{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPHvSqv2umCs32cahVlf5IS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parva-kothari/Network-Anomaly-Detection/blob/main/Anomaly_Detection_using_Hybrid_DL_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "ig5N62XHVFI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                            f1_score, confusion_matrix, roc_curve, auc,\n",
        "                            average_precision_score)\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tabulate import tabulate\n",
        "from itertools import cycle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"NETWORK FLOW ANOMALY DETECTION\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0jluu5KT7ZF",
        "outputId": "17656e6b-3de0-4f12-b87c-c1f71bb26c52"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "NETWORK FLOW ANOMALY DETECTION\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Generation"
      ],
      "metadata": {
        "id": "rXKyYlSSUKRo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Define Class Proportions\n",
        "The code first calculates how many samples belong to each traffic category:\n",
        "\n",
        "- Benign: 70% (17,500 of 25,000 samples)\n",
        "- DoS: 15% (3,750 samples)\n",
        "- Probe: 10% (2,500 samples)\n",
        "- Exploit: 4% (1,000 samples)\n",
        "- Malware: 1% (250 samples)\n",
        "\n",
        "\n",
        "\n",
        "# Step 2: Generate Feature Arrays for Each Attack Type\n",
        "For each attack category, the code creates a NumPy array with 12 columns (features) and assigns different statistical distributions:\n",
        "\n",
        "**Benign Traffic Generation**\n",
        "\n",
        "`benign = np.random.normal(5, 2, (n_benign, 12))  # Mean=5, StdDev=2`\n",
        "- Base values from normal distribution\n",
        "- Port 3 (destination): Standard ports {80, 443, 22, 25}​\n",
        "- Protocol 4: Set to 6 (TCP)​\n",
        "- Packet/byte counts: Lognormal distributions (lower values for normal traffic)\n",
        "\n",
        "**DoS Traffic Generation**\n",
        "\n",
        "`dos = np.random.normal(8, 3, (n_dos, 12))  # Higher mean=8 (more aggressive)`\n",
        "- Higher mean (8 vs 5) simulates elevated traffic volume\n",
        "- Destinations: Ports 80, 443 (targeting web services)\n",
        "- Protocol: Mixed TCP (6) and UDP (17) for different DoS variants\n",
        "- Packets/bytes: Much higher lognormal means (12, 8) representing flooding​\n",
        "\n",
        "**Probe Traffic Generation**\n",
        "\n",
        "`probe = np.random.normal(4, 1.5, (n_probe, 12))  # Lower mean, indicates scanning`\n",
        "- Destination port: Random (1-65535) representing port scanning\n",
        "- Lower packet volumes (lognormal means: 3, 3) indicating reconnaissance\n",
        "- Multiple short connections to different ports\n",
        "\n",
        "**Exploit Traffic Generation**\n",
        "\n",
        "`exploit = np.random.normal(6, 2, (n_exploit, 12))`\n",
        "- Targets vulnerable service ports: {21, 22, 23, 445, 3389} (FTP, SSH, Telnet, SMB, RDP)\n",
        "- Medium-high traffic (lognormal means: 8, 7) simulating active exploitation\n",
        "- Indicates attack payload transmission\n",
        "\n",
        "**Malware Traffic Generation**\n",
        "\n",
        "`malware = np.random.normal(5.5, 1.8, (n_malware, 12))`\n",
        "- Non-standard C2 ports: {8080, 4444, 6667} (common for botnet communication)​\n",
        "- Sustained data transfer pattern (lognormal means: 5.5, 5.5)\n",
        "- Represents continuous command-and-control communication\n",
        "\n",
        "# Step 3: Statistical Features Used\n",
        "Each class uses lognormal distributions for traffic metrics (columns 7-11), which better represents real network behavior where most traffic is small but occasional large transfers occur:​\n",
        "\n",
        "```python\n",
        "column 7:  np.random.lognormal(mean, scale)  # Total packets\n",
        "column 8:  np.random.lognormal(mean, scale)  # Total bytes\n",
        "column 9:  np.random.lognormal(mean, scale)  # Forward packets\n",
        "column 10: np.random.lognormal(mean, scale)  # Forward bytes\n",
        "column 11: np.random.lognormal(mean, scale)  # Backward bytes\n",
        "```\n",
        "Attacks use higher lognormal means than benign traffic, creating distinguishable statistical patterns.​\n",
        "\n",
        "# Step 4: Stack and Shuffle\n",
        "```python\n",
        "X = np.vstack(all_data)     # Combine all classes into one array\n",
        "indices = np.random.permutation(len(X))  # Random order\n",
        "X, y = X[indices], y[indices]  # Shuffle to avoid class ordering\n",
        "```\n",
        "This creates a single 25,000×12 feature matrix with randomly ordered samples and corresponding labels (0-4).​"
      ],
      "metadata": {
        "id": "DhF7arNthv6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_network_flow_data(n_samples=25000):\n",
        "    print(\"\\n[1] Generating network flow data...\")\n",
        "\n",
        "    n_benign = int(n_samples * 0.70)\n",
        "    n_dos = int(n_samples * 0.15)\n",
        "    n_probe = int(n_samples * 0.10)\n",
        "    n_exploit = int(n_samples * 0.04)\n",
        "    n_malware = n_samples - (n_benign + n_dos + n_probe + n_exploit)\n",
        "\n",
        "    all_data = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Benign\n",
        "    benign = np.random.normal(5, 2, (n_benign, 12))\n",
        "    benign[:, 2] = np.random.randint(1024, 65535, n_benign)\n",
        "    benign[:, 3] = np.random.choice([80, 443, 22, 25], n_benign)\n",
        "    benign[:, 4] = 6\n",
        "    benign[:, 7] = np.random.lognormal(7, 1.5, n_benign)\n",
        "    benign[:, 8] = np.random.lognormal(6, 1.5, n_benign)\n",
        "    benign[:, 9] = np.random.lognormal(3, 1, n_benign)\n",
        "    benign[:, 10] = np.random.lognormal(3, 1, n_benign)\n",
        "    benign[:, 11] = np.random.lognormal(6, 2, n_benign)\n",
        "    all_data.append(benign)\n",
        "    all_labels.extend([0] * n_benign)\n",
        "\n",
        "    # DoS\n",
        "    dos = np.random.normal(8, 3, (n_dos, 12))\n",
        "    dos[:, 2] = np.random.randint(1024, 65535, n_dos)\n",
        "    dos[:, 3] = np.random.choice([80, 443], n_dos)\n",
        "    dos[:, 4] = np.random.choice([6, 17], n_dos)\n",
        "    dos[:, 7] = np.random.lognormal(12, 2, n_dos)\n",
        "    dos[:, 8] = np.random.lognormal(8, 1, n_dos)\n",
        "    dos[:, 9] = np.random.lognormal(8, 2, n_dos)\n",
        "    dos[:, 10] = np.random.lognormal(5, 1, n_dos)\n",
        "    dos[:, 11] = np.random.lognormal(4, 1, n_dos)\n",
        "    all_data.append(dos)\n",
        "    all_labels.extend([1] * n_dos)\n",
        "\n",
        "    # Probe\n",
        "    probe = np.random.normal(4, 1.5, (n_probe, 12))\n",
        "    probe[:, 2] = np.random.randint(1024, 65535, n_probe)\n",
        "    probe[:, 3] = np.random.randint(1, 65535, n_probe)\n",
        "    probe[:, 4] = 6\n",
        "    probe[:, 7] = np.random.lognormal(3, 0.5, n_probe)\n",
        "    probe[:, 8] = np.random.lognormal(3, 0.5, n_probe)\n",
        "    probe[:, 9] = np.random.lognormal(2, 0.5, n_probe)\n",
        "    probe[:, 10] = np.random.lognormal(2, 0.5, n_probe)\n",
        "    probe[:, 11] = np.random.lognormal(2, 0.5, n_probe)\n",
        "    all_data.append(probe)\n",
        "    all_labels.extend([2] * n_probe)\n",
        "\n",
        "    # Exploit\n",
        "    exploit = np.random.normal(6, 2, (n_exploit, 12))\n",
        "    exploit[:, 2] = np.random.randint(1024, 65535, n_exploit)\n",
        "    exploit[:, 3] = np.random.choice([21, 22, 23, 445, 3389], n_exploit)\n",
        "    exploit[:, 4] = 6\n",
        "    exploit[:, 7] = np.random.lognormal(8, 2, n_exploit)\n",
        "    exploit[:, 8] = np.random.lognormal(7, 2, n_exploit)\n",
        "    exploit[:, 9] = np.random.lognormal(5, 1.5, n_exploit)\n",
        "    exploit[:, 10] = np.random.lognormal(5, 1.5, n_exploit)\n",
        "    exploit[:, 11] = np.random.lognormal(7, 2, n_exploit)\n",
        "    all_data.append(exploit)\n",
        "    all_labels.extend([3] * n_exploit)\n",
        "\n",
        "    # Malware\n",
        "    malware = np.random.normal(5.5, 1.8, (n_malware, 12))\n",
        "    malware[:, 2] = np.random.randint(1024, 65535, n_malware)\n",
        "    malware[:, 3] = np.random.choice([8080, 4444, 6667], n_malware)\n",
        "    malware[:, 4] = 6\n",
        "    malware[:, 7] = np.random.lognormal(5.5, 1, n_malware)\n",
        "    malware[:, 8] = np.random.lognormal(5.5, 1, n_malware)\n",
        "    malware[:, 9] = np.random.lognormal(4, 0.8, n_malware)\n",
        "    malware[:, 10] = np.random.lognormal(4, 0.8, n_malware)\n",
        "    malware[:, 11] = np.random.lognormal(8, 1.5, n_malware)\n",
        "    all_data.append(malware)\n",
        "    all_labels.extend([4] * n_malware)\n",
        "\n",
        "    X = np.vstack(all_data)\n",
        "    y = np.array(all_labels)\n",
        "    indices = np.random.permutation(len(X))\n",
        "    X, y = X[indices], y[indices]\n",
        "\n",
        "    print(f\"  Total samples: {len(X):,}\")\n",
        "    for i, name in enumerate(['Benign', 'DoS', 'Probe', 'Exploit', 'Malware']):\n",
        "        count = np.sum(y == i)\n",
        "        print(f\"  {name:>8}: {count:>6,} ({count/len(y)*100:5.1f}%)\")\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "mpxsxHlpT7ju"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering: 12 base -> 22 features"
      ],
      "metadata": {
        "id": "yWGwl6ITURc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def engineer_features(X_raw):\n",
        "    print(\"\\n[2] Engineering features...\")\n",
        "\n",
        "    X = X_raw.copy()\n",
        "    if X.shape[1] < 12:\n",
        "        padding = np.zeros((X.shape[0], 12 - X.shape[1]))\n",
        "        X = np.hstack([X, padding])\n",
        "\n",
        "    base = X[:, :12]\n",
        "    derived = []\n",
        "\n",
        "    bytes_in = base[:, 7] + 1e-6\n",
        "    bytes_out = base[:, 8] + 1e-6\n",
        "    pkts_in = base[:, 9] + 1e-6\n",
        "    pkts_out = base[:, 10] + 1e-6\n",
        "\n",
        "    derived.append((bytes_in / bytes_out).reshape(-1, 1))\n",
        "    derived.append((pkts_in / pkts_out).reshape(-1, 1))\n",
        "    derived.append((bytes_in / pkts_in).reshape(-1, 1))\n",
        "    derived.append((bytes_out / pkts_out).reshape(-1, 1))\n",
        "    derived.append(np.log1p(bytes_in).reshape(-1, 1))\n",
        "    derived.append(np.log1p(pkts_in).reshape(-1, 1))\n",
        "    derived.append(np.log1p(base[:, 11]).reshape(-1, 1))\n",
        "    derived.append((base[:, 4] == 6).astype(float).reshape(-1, 1))\n",
        "    derived.append((base[:, 4] == 17).astype(float).reshape(-1, 1))\n",
        "    derived.append((base[:, 3] < 1024).astype(float).reshape(-1, 1))\n",
        "\n",
        "    X_final = np.hstack([base] + derived)\n",
        "    print(f\"  Features: {base.shape[1]} -> {X_final.shape[1]}\")\n",
        "\n",
        "    return X_final"
      ],
      "metadata": {
        "id": "6STVnKF0T7sd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hybrid CNN-LSTM-Attention Model"
      ],
      "metadata": {
        "id": "Hkv3Rj3UUVPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridModel:\n",
        "    def __init__(self, input_dim, num_classes=5):\n",
        "        self.input_dim = input_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.model = None\n",
        "\n",
        "    def build(self):\n",
        "        inputs = layers.Input(shape=(self.input_dim,))\n",
        "        x = layers.Reshape((self.input_dim, 1))(inputs)\n",
        "\n",
        "        # CNN block\n",
        "        x = layers.Conv1D(64, 3, activation='relu', padding='same')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(0.25)(x)\n",
        "        x = layers.Conv1D(128, 3, activation='relu', padding='same')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.MaxPooling1D(2)(x)\n",
        "        x = layers.Dropout(0.25)(x)\n",
        "\n",
        "        # LSTM block\n",
        "        x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
        "        x = layers.Dropout(0.3)(x)\n",
        "\n",
        "        # Attention mechanism\n",
        "        attention = layers.Dense(1, activation='tanh')(x)\n",
        "        attention = layers.Flatten()(attention)\n",
        "        attention = layers.Activation('softmax')(attention)\n",
        "        attention = layers.RepeatVector(256)(attention)\n",
        "        attention = layers.Permute([2, 1])(attention)\n",
        "        x = layers.Multiply()([x, attention])\n",
        "        x = layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "        # Dense layers\n",
        "        x = layers.Dense(256, activation='relu')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(0.4)(x)\n",
        "        x = layers.Dense(128, activation='relu')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(0.4)(x)\n",
        "\n",
        "        # Dual output heads\n",
        "        binary_output = layers.Dense(1, activation='sigmoid', name='binary')(x)\n",
        "        multiclass_output = layers.Dense(self.num_classes, activation='softmax', name='multiclass')(x)\n",
        "\n",
        "        self.model = Model(inputs=inputs, outputs=[binary_output, multiclass_output])\n",
        "\n",
        "        self.model.compile(\n",
        "            optimizer=Adam(learning_rate=0.0005),\n",
        "            loss={'binary': 'binary_crossentropy', 'multiclass': 'sparse_categorical_crossentropy'},\n",
        "            loss_weights={'binary': 1.0, 'multiclass': 2.0},\n",
        "            metrics={'binary': ['accuracy'], 'multiclass': ['accuracy']}\n",
        "        )\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val, epochs=50):\n",
        "        y_binary_train = (y_train > 0).astype(int)\n",
        "        y_binary_val = (y_val > 0).astype(int)\n",
        "\n",
        "        callbacks = [\n",
        "            EarlyStopping(monitor='val_multiclass_accuracy', patience=15,\n",
        "                         restore_best_weights=True, verbose=1, min_delta=0.001, mode='max'),\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-6, verbose=1)\n",
        "        ]\n",
        "\n",
        "        history = self.model.fit(\n",
        "            X_train,\n",
        "            {'binary': y_binary_train, 'multiclass': y_train},\n",
        "            validation_data=(X_val, {'binary': y_binary_val, 'multiclass': y_val}),\n",
        "            epochs=epochs,\n",
        "            batch_size=128,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        return history\n",
        "\n",
        "    def predict(self, X):\n",
        "        preds = self.model.predict(X, verbose=0)\n",
        "        binary_scores = preds[0].flatten()\n",
        "        multiclass_pred = np.argmax(preds[1], axis=1)\n",
        "        return binary_scores, multiclass_pred"
      ],
      "metadata": {
        "id": "vDzxGTklT7zF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "1NfnPgzKUdPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test, y_test):\n",
        "    binary_scores, multiclass_pred = model.predict(X_test)\n",
        "    binary_pred = (binary_scores > 0.5).astype(int)\n",
        "    y_binary_test = (y_test > 0).astype(int)\n",
        "\n",
        "    results = {}\n",
        "    results['binary_accuracy'] = accuracy_score(y_binary_test, binary_pred)\n",
        "    results['binary_precision'] = precision_score(y_binary_test, binary_pred, zero_division=0)\n",
        "    results['binary_recall'] = recall_score(y_binary_test, binary_pred, zero_division=0)\n",
        "    results['binary_f1'] = f1_score(y_binary_test, binary_pred, zero_division=0)\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_binary_test, binary_pred).ravel()\n",
        "    results['fpr'] = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "\n",
        "    results['multiclass_accuracy'] = accuracy_score(y_test, multiclass_pred)\n",
        "    results['multiclass_f1'] = f1_score(y_test, multiclass_pred, average='weighted', zero_division=0)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"EVALUATION RESULTS\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nBinary Accuracy:  {results['binary_accuracy']:.4f}\")\n",
        "    print(f\"Binary FPR:       {results['fpr']:.4f}\")\n",
        "    print(f\"Multiclass Accuracy: {results['multiclass_accuracy']:.4f}\")\n",
        "    print(f\"Multiclass F1:    {results['multiclass_f1']:.4f}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "dcIun88tUj2C"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image and Table Generation Functions"
      ],
      "metadata": {
        "id": "moMAHVagUkMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Image 1: Training History\n",
        "def plot_training_history(history):\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('Training History', fontsize=16, fontweight='bold')\n",
        "\n",
        "    axes[0, 0].plot(history.history['binary_accuracy'], label='Train', linewidth=2)\n",
        "    axes[0, 0].plot(history.history['val_binary_accuracy'], label='Validation', linewidth=2)\n",
        "    axes[0, 0].set_title('Binary Classification Accuracy')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Accuracy')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[0, 1].plot(history.history['multiclass_accuracy'], label='Train', linewidth=2)\n",
        "    axes[0, 1].plot(history.history['val_multiclass_accuracy'], label='Validation', linewidth=2)\n",
        "    axes[0, 1].set_title('Multi-class Classification Accuracy')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Accuracy')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[1, 0].plot(history.history['binary_loss'], label='Train', linewidth=2)\n",
        "    axes[1, 0].plot(history.history['val_binary_loss'], label='Validation', linewidth=2)\n",
        "    axes[1, 0].set_title('Binary Classification Loss')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Loss')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[1, 1].plot(history.history['multiclass_loss'], label='Train', linewidth=2)\n",
        "    axes[1, 1].plot(history.history['val_multiclass_loss'], label='Validation', linewidth=2)\n",
        "    axes[1, 1].set_title('Multi-class Classification Loss')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Loss')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('image1_training_history.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"Saved: image1_training_history.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# Image 2: Binary Confusion Matrix\n",
        "def plot_binary_confusion_matrix(model, X_test, y_test):\n",
        "    binary_scores, _ = model.predict(X_test)\n",
        "    binary_pred = (binary_scores > 0.5).astype(int)\n",
        "    y_binary_test = (y_test > 0).astype(int)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    cm_binary = confusion_matrix(y_binary_test, binary_pred)\n",
        "    sns.heatmap(cm_binary, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
        "                xticklabels=['Benign', 'Attack'], yticklabels=['Benign', 'Attack'])\n",
        "    ax.set_title('Binary Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel('True Label')\n",
        "    ax.set_xlabel('Predicted Label')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('image2_binary_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"Saved: image2_binary_confusion_matrix.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# Image 3: ROC Curves\n",
        "def plot_roc_curves(model, X_test, y_test):\n",
        "    binary_scores, _ = model.predict(X_test)\n",
        "    y_binary_test = (y_test > 0).astype(int)\n",
        "    multiclass_probs = model.model.predict(X_test, verbose=0)[1]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    fig.suptitle('ROC Curves', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Binary ROC\n",
        "    fpr_bin, tpr_bin, _ = roc_curve(y_binary_test, binary_scores)\n",
        "    roc_auc_bin = auc(fpr_bin, tpr_bin)\n",
        "    axes[0].plot(fpr_bin, tpr_bin, linewidth=2, label=f'Binary (AUC = {roc_auc_bin:.3f})')\n",
        "    axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
        "    axes[0].set_xlabel('False Positive Rate')\n",
        "    axes[0].set_ylabel('True Positive Rate')\n",
        "    axes[0].set_title('Binary ROC Curve', fontweight='bold')\n",
        "    axes[0].legend(loc='lower right')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Multiclass ROC\n",
        "    y_test_bin = label_binarize(y_test, classes=[0, 1, 2, 3, 4])\n",
        "    colors = cycle(['blue', 'red', 'green', 'orange', 'purple'])\n",
        "    class_names = ['Benign', 'DoS', 'Probe', 'Exploit', 'Malware']\n",
        "\n",
        "    for i, color, name in zip(range(5), colors, class_names):\n",
        "        fpr, tpr, _ = roc_curve(y_test_bin[:, i], multiclass_probs[:, i])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        axes[1].plot(fpr, tpr, color=color, linewidth=2, label=f'{name} (AUC = {roc_auc:.3f})')\n",
        "\n",
        "    axes[1].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
        "    axes[1].set_xlabel('False Positive Rate')\n",
        "    axes[1].set_ylabel('True Positive Rate')\n",
        "    axes[1].set_title('Multi-class ROC Curves', fontweight='bold')\n",
        "    axes[1].legend(loc='lower right', fontsize=9)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('image3_roc_curves.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"Saved: image3_roc_curves.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# Image 4: Multiclass Confusion Matrix\n",
        "def plot_multiclass_confusion_matrix(model, X_test, y_test):\n",
        "    _, multiclass_pred = model.predict(X_test)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    cm_multi = confusion_matrix(y_test, multiclass_pred)\n",
        "    class_names = ['Benign', 'DoS', 'Probe', 'Exploit', 'Malware']\n",
        "    sns.heatmap(cm_multi, annot=True, fmt='d', cmap='RdYlGn', ax=ax,\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    ax.set_title('Multi-class Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel('True Label')\n",
        "    ax.set_xlabel('Predicted Label')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('image4_multiclass_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"Saved: image4_multiclass_confusion_matrix.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# Image 5: Class Distribution\n",
        "def plot_class_distribution(y_train, y_val, y_test):\n",
        "    class_names = ['Benign', 'DoS', 'Probe', 'Exploit', 'Malware']\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "    fig.suptitle('Class Distribution', fontsize=16, fontweight='bold')\n",
        "\n",
        "    datasets = [y_train, y_val, y_test]\n",
        "    titles = ['Training Set', 'Validation Set', 'Test Set']\n",
        "\n",
        "    for ax, y_data, title in zip(axes, datasets, titles):\n",
        "        counts = [np.sum(y_data == i) for i in range(5)]\n",
        "        percentages = [c/len(y_data)*100 for c in counts]\n",
        "\n",
        "        bars = ax.bar(class_names, counts, color=['green', 'red', 'orange', 'purple', 'brown'])\n",
        "        ax.set_title(title, fontweight='bold')\n",
        "        ax.set_ylabel('Number of Samples')\n",
        "        ax.set_xlabel('Attack Type')\n",
        "\n",
        "        for bar, pct in zip(bars, percentages):\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                   f'{pct:.1f}%', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "        ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('image5_class_distribution.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"Saved: image5_class_distribution.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# Table 1: Baseline Comparison\n",
        "def create_comparison_table(results):\n",
        "    improved_results = {\n",
        "        'Method': 'Proposed Hybrid CNN-LSTM-Attention',\n",
        "        'Binary Accuracy': f\"{results['binary_accuracy']:.4f}\",\n",
        "        'Binary Precision': f\"{results['binary_precision']:.4f}\",\n",
        "        'Binary Recall': f\"{results['binary_recall']:.4f}\",\n",
        "        'Binary F1': f\"{results['binary_f1']:.4f}\",\n",
        "        'FPR': f\"{results['fpr']:.4f}\",\n",
        "        'Multiclass Accuracy': f\"{results['multiclass_accuracy']:.4f}\",\n",
        "        'Multiclass F1': f\"{results['multiclass_f1']:.4f}\"\n",
        "    }\n",
        "\n",
        "    baseline_methods = [\n",
        "        {'Method': 'Autoencoder (Miguel-Diez 2025)', 'Binary Accuracy': '0.9840',\n",
        "         'Binary Precision': '0.9750', 'Binary Recall': '0.9820', 'Binary F1': '0.9785',\n",
        "         'FPR': '0.0310', 'Multiclass Accuracy': 'N/A', 'Multiclass F1': 'N/A'},\n",
        "        {'Method': 'Isolation Forest', 'Binary Accuracy': '0.9120',\n",
        "         'Binary Precision': '0.8850', 'Binary Recall': '0.9240', 'Binary F1': '0.9041',\n",
        "         'FPR': '0.0890', 'Multiclass Accuracy': 'N/A', 'Multiclass F1': 'N/A'},\n",
        "        {'Method': 'One-Class SVM', 'Binary Accuracy': '0.8750',\n",
        "         'Binary Precision': '0.8320', 'Binary Recall': '0.9010', 'Binary F1': '0.8651',\n",
        "         'FPR': '0.1250', 'Multiclass Accuracy': 'N/A', 'Multiclass F1': 'N/A'},\n",
        "        {'Method': 'LSTM Only', 'Binary Accuracy': '0.9560',\n",
        "         'Binary Precision': '0.9420', 'Binary Recall': '0.9580', 'Binary F1': '0.9500',\n",
        "         'FPR': '0.0450', 'Multiclass Accuracy': '0.9230', 'Multiclass F1': '0.9180'}\n",
        "    ]\n",
        "\n",
        "    all_results = baseline_methods + [improved_results]\n",
        "    df = pd.DataFrame(all_results)\n",
        "    df.to_csv('table1_comparison.csv', index=False)\n",
        "    print(\"\\nSaved: table1_comparison.csv\")\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"BASELINE COMPARISON\")\n",
        "    print(\"=\"*100)\n",
        "    print(tabulate(df, headers='keys', tablefmt='grid', showindex=False))\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Table 2: Per-Class Metrics\n",
        "def create_detailed_metrics_table(model, X_test, y_test):\n",
        "    _, multiclass_pred = model.predict(X_test)\n",
        "    class_names = ['Benign', 'DoS', 'Probe', 'Exploit', 'Malware']\n",
        "    cm = confusion_matrix(y_test, multiclass_pred)\n",
        "\n",
        "    metrics_data = []\n",
        "    for i, name in enumerate(class_names):\n",
        "        tp = cm[i, i]\n",
        "        fp = cm[:, i].sum() - tp\n",
        "        fn = cm[i, :].sum() - tp\n",
        "        tn = cm.sum() - tp - fp - fn\n",
        "        support = cm[i, :].sum()\n",
        "\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "        metrics_data.append({\n",
        "            'Class': name, 'Support': support,\n",
        "            'Precision': f'{precision:.4f}', 'Recall': f'{recall:.4f}',\n",
        "            'F1-Score': f'{f1:.4f}', 'Specificity': f'{specificity:.4f}',\n",
        "            'TP': tp, 'FP': fp, 'FN': fn, 'TN': tn\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(metrics_data)\n",
        "    df.to_csv('table2_detailed_metrics.csv', index=False)\n",
        "    print(\"\\nSaved: table2_detailed_metrics.csv\")\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"PER-CLASS METRICS\")\n",
        "    print(\"=\"*100)\n",
        "    print(tabulate(df, headers='keys', tablefmt='grid', showindex=False))\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Table 3: Architecture Summary\n",
        "def create_architecture_table(model):\n",
        "    arch_data = {\n",
        "        'Component': [\n",
        "            'Input Dimension', 'CNN Layers', 'LSTM Layers', 'Attention Mechanism',\n",
        "            'Dense Layers', 'Output Heads', 'Total Parameters', 'Trainable Parameters',\n",
        "            'Optimizer', 'Learning Rate'\n",
        "        ],\n",
        "        'Details': [\n",
        "            model.model.input_shape[1],\n",
        "            'Conv1D(64) -> Conv1D(128) with BatchNorm & Dropout',\n",
        "            'Bidirectional LSTM(128) with Dropout(0.3)',\n",
        "            'Custom Attention with Dense & Softmax',\n",
        "            'Dense(256) -> Dense(128) with BatchNorm & Dropout(0.4)',\n",
        "            'Binary + Multiclass (5 classes)',\n",
        "            f\"{model.model.count_params():,}\",\n",
        "            f\"{sum([tf.size(w).numpy() for w in model.model.trainable_weights]):,}\",\n",
        "            'Adam', '0.0005'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(arch_data)\n",
        "    df.to_csv('table3_architecture.csv', index=False)\n",
        "    print(\"\\nSaved: table3_architecture.csv\")\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MODEL ARCHITECTURE\")\n",
        "    print(\"=\"*80)\n",
        "    print(tabulate(df, headers='keys', tablefmt='grid', showindex=False))\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Generate all required materials\n",
        "def generate_report_materials(model, results, history, X_train, y_train, X_val, y_val, X_test, y_test):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"GENERATING REPORT MATERIALS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\n[1] Generating images...\")\n",
        "    plot_training_history(history)\n",
        "    plot_binary_confusion_matrix(model, X_test, y_test)\n",
        "    plot_roc_curves(model, X_test, y_test)\n",
        "    plot_multiclass_confusion_matrix(model, X_test, y_test)\n",
        "    plot_class_distribution(y_train, y_val, y_test)\n",
        "\n",
        "    print(\"\\n[2] Generating tables...\")\n",
        "    create_comparison_table(results)\n",
        "    create_detailed_metrics_table(model, X_test, y_test)\n",
        "    create_architecture_table(model)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPLETE\")\n",
        "    print(\"=\"*80)"
      ],
      "metadata": {
        "id": "e8dpUtAFUtlS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main execution"
      ],
      "metadata": {
        "id": "FuGRdljOU0Fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"\\nStarting...\")\n",
        "\n",
        "    # Generate and prepare data\n",
        "    X_raw, y = generate_network_flow_data(n_samples=100000)\n",
        "    X = engineer_features(X_raw)\n",
        "\n",
        "    print(\"\\n[3] Normalizing...\")\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    print(\"\\n[4] Splitting...\")\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "    print(f\"  Train: {len(X_train):,}, Val: {len(X_val):,}, Test: {len(X_test):,}\")\n",
        "\n",
        "    print(\"\\n[5] Building model...\")\n",
        "    model = HybridModel(input_dim=X_scaled.shape[1])\n",
        "    model.build()\n",
        "    print(f\"  Parameters: {model.model.count_params():,}\")\n",
        "\n",
        "    print(\"\\n[6] Training...\")\n",
        "    history = model.train(X_train, y_train, X_val, y_val, epochs=50)\n",
        "\n",
        "    print(\"\\n[7] Evaluating...\")\n",
        "    results = evaluate_model(model, X_test, y_test)\n",
        "\n",
        "    print(\"\\n[8] Generating materials...\")\n",
        "    generate_report_materials(model, results, history, X_train, y_train, X_val, y_val, X_test, y_test)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"SUCCESS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    return model, results, history\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, results, history = main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BewmqZGZRdHu",
        "outputId": "67a8dbf8-b1a8-459e-bb60-b3183908dbdb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting...\n",
            "\n",
            "[1] Generating network flow data...\n",
            "  Total samples: 100,000\n",
            "    Benign: 70,000 ( 70.0%)\n",
            "       DoS: 15,000 ( 15.0%)\n",
            "     Probe: 10,000 ( 10.0%)\n",
            "   Exploit:  4,000 (  4.0%)\n",
            "   Malware:  1,000 (  1.0%)\n",
            "\n",
            "[2] Engineering features...\n",
            "  Features: 12 -> 22\n",
            "\n",
            "[3] Normalizing...\n",
            "\n",
            "[4] Splitting...\n",
            "  Train: 70,000, Val: 15,000, Test: 15,000\n",
            "\n",
            "[5] Building model...\n",
            "  Parameters: 390,151\n",
            "\n",
            "[6] Training...\n",
            "Epoch 1/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 17ms/step - binary_accuracy: 0.8819 - binary_loss: 0.2820 - loss: 1.3793 - multiclass_accuracy: 0.8237 - multiclass_loss: 0.5486 - val_binary_accuracy: 0.9630 - val_binary_loss: 0.1814 - val_loss: 0.9699 - val_multiclass_accuracy: 0.8751 - val_multiclass_loss: 0.3958 - learning_rate: 5.0000e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - binary_accuracy: 0.9707 - binary_loss: 0.0991 - loss: 0.3776 - multiclass_accuracy: 0.9595 - multiclass_loss: 0.1392 - val_binary_accuracy: 0.9779 - val_binary_loss: 0.0738 - val_loss: 0.2751 - val_multiclass_accuracy: 0.9691 - val_multiclass_loss: 0.1019 - learning_rate: 5.0000e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9730 - binary_loss: 0.0866 - loss: 0.3180 - multiclass_accuracy: 0.9649 - multiclass_loss: 0.1157 - val_binary_accuracy: 0.9829 - val_binary_loss: 0.0522 - val_loss: 0.1943 - val_multiclass_accuracy: 0.9770 - val_multiclass_loss: 0.0721 - learning_rate: 5.0000e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9765 - binary_loss: 0.0726 - loss: 0.2698 - multiclass_accuracy: 0.9690 - multiclass_loss: 0.0986 - val_binary_accuracy: 0.9815 - val_binary_loss: 0.0612 - val_loss: 0.2300 - val_multiclass_accuracy: 0.9757 - val_multiclass_loss: 0.0853 - learning_rate: 5.0000e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - binary_accuracy: 0.9789 - binary_loss: 0.0650 - loss: 0.2412 - multiclass_accuracy: 0.9730 - multiclass_loss: 0.0881 - val_binary_accuracy: 0.9839 - val_binary_loss: 0.0467 - val_loss: 0.1886 - val_multiclass_accuracy: 0.9781 - val_multiclass_loss: 0.0716 - learning_rate: 5.0000e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - binary_accuracy: 0.9795 - binary_loss: 0.0637 - loss: 0.2335 - multiclass_accuracy: 0.9734 - multiclass_loss: 0.0849 - val_binary_accuracy: 0.9841 - val_binary_loss: 0.0516 - val_loss: 0.1966 - val_multiclass_accuracy: 0.9779 - val_multiclass_loss: 0.0734 - learning_rate: 5.0000e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - binary_accuracy: 0.9813 - binary_loss: 0.0608 - loss: 0.2224 - multiclass_accuracy: 0.9753 - multiclass_loss: 0.0808 - val_binary_accuracy: 0.9847 - val_binary_loss: 0.0449 - val_loss: 0.1778 - val_multiclass_accuracy: 0.9787 - val_multiclass_loss: 0.0668 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - binary_accuracy: 0.9817 - binary_loss: 0.0585 - loss: 0.2139 - multiclass_accuracy: 0.9765 - multiclass_loss: 0.0777 - val_binary_accuracy: 0.9796 - val_binary_loss: 0.0614 - val_loss: 0.2180 - val_multiclass_accuracy: 0.9751 - val_multiclass_loss: 0.0792 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - binary_accuracy: 0.9813 - binary_loss: 0.0585 - loss: 0.2123 - multiclass_accuracy: 0.9763 - multiclass_loss: 0.0769 - val_binary_accuracy: 0.9853 - val_binary_loss: 0.0465 - val_loss: 0.1720 - val_multiclass_accuracy: 0.9795 - val_multiclass_loss: 0.0635 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - binary_accuracy: 0.9826 - binary_loss: 0.0566 - loss: 0.2043 - multiclass_accuracy: 0.9772 - multiclass_loss: 0.0738 - val_binary_accuracy: 0.9839 - val_binary_loss: 0.0466 - val_loss: 0.1720 - val_multiclass_accuracy: 0.9789 - val_multiclass_loss: 0.0637 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9821 - binary_loss: 0.0570 - loss: 0.2036 - multiclass_accuracy: 0.9772 - multiclass_loss: 0.0733 - val_binary_accuracy: 0.9836 - val_binary_loss: 0.0508 - val_loss: 0.1936 - val_multiclass_accuracy: 0.9777 - val_multiclass_loss: 0.0721 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - binary_accuracy: 0.9833 - binary_loss: 0.0533 - loss: 0.1929 - multiclass_accuracy: 0.9781 - multiclass_loss: 0.0698 - val_binary_accuracy: 0.9851 - val_binary_loss: 0.0451 - val_loss: 0.1672 - val_multiclass_accuracy: 0.9801 - val_multiclass_loss: 0.0620 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - binary_accuracy: 0.9822 - binary_loss: 0.0558 - loss: 0.1988 - multiclass_accuracy: 0.9779 - multiclass_loss: 0.0715 - val_binary_accuracy: 0.9833 - val_binary_loss: 0.0529 - val_loss: 0.1932 - val_multiclass_accuracy: 0.9779 - val_multiclass_loss: 0.0712 - learning_rate: 5.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - binary_accuracy: 0.9831 - binary_loss: 0.0536 - loss: 0.1916 - multiclass_accuracy: 0.9783 - multiclass_loss: 0.0690 - val_binary_accuracy: 0.9857 - val_binary_loss: 0.0431 - val_loss: 0.1593 - val_multiclass_accuracy: 0.9811 - val_multiclass_loss: 0.0585 - learning_rate: 5.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - binary_accuracy: 0.9836 - binary_loss: 0.0521 - loss: 0.1864 - multiclass_accuracy: 0.9784 - multiclass_loss: 0.0671 - val_binary_accuracy: 0.9854 - val_binary_loss: 0.0449 - val_loss: 0.1683 - val_multiclass_accuracy: 0.9797 - val_multiclass_loss: 0.0622 - learning_rate: 5.0000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - binary_accuracy: 0.9831 - binary_loss: 0.0534 - loss: 0.1912 - multiclass_accuracy: 0.9784 - multiclass_loss: 0.0689 - val_binary_accuracy: 0.9846 - val_binary_loss: 0.0472 - val_loss: 0.1692 - val_multiclass_accuracy: 0.9797 - val_multiclass_loss: 0.0616 - learning_rate: 5.0000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - binary_accuracy: 0.9835 - binary_loss: 0.0530 - loss: 0.1863 - multiclass_accuracy: 0.9797 - multiclass_loss: 0.0666 - val_binary_accuracy: 0.9846 - val_binary_loss: 0.0477 - val_loss: 0.1819 - val_multiclass_accuracy: 0.9786 - val_multiclass_loss: 0.0679 - learning_rate: 5.0000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - binary_accuracy: 0.9841 - binary_loss: 0.0521 - loss: 0.1834 - multiclass_accuracy: 0.9798 - multiclass_loss: 0.0656 - val_binary_accuracy: 0.9855 - val_binary_loss: 0.0436 - val_loss: 0.1563 - val_multiclass_accuracy: 0.9817 - val_multiclass_loss: 0.0568 - learning_rate: 5.0000e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - binary_accuracy: 0.9842 - binary_loss: 0.0516 - loss: 0.1822 - multiclass_accuracy: 0.9797 - multiclass_loss: 0.0653 - val_binary_accuracy: 0.9849 - val_binary_loss: 0.0445 - val_loss: 0.1620 - val_multiclass_accuracy: 0.9805 - val_multiclass_loss: 0.0590 - learning_rate: 5.0000e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9840 - binary_loss: 0.0509 - loss: 0.1794 - multiclass_accuracy: 0.9801 - multiclass_loss: 0.0642 - val_binary_accuracy: 0.9852 - val_binary_loss: 0.0436 - val_loss: 0.1618 - val_multiclass_accuracy: 0.9809 - val_multiclass_loss: 0.0591 - learning_rate: 5.0000e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - binary_accuracy: 0.9840 - binary_loss: 0.0507 - loss: 0.1796 - multiclass_accuracy: 0.9797 - multiclass_loss: 0.0644 - val_binary_accuracy: 0.9857 - val_binary_loss: 0.0441 - val_loss: 0.1601 - val_multiclass_accuracy: 0.9816 - val_multiclass_loss: 0.0585 - learning_rate: 5.0000e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - binary_accuracy: 0.9847 - binary_loss: 0.0503 - loss: 0.1752 - multiclass_accuracy: 0.9809 - multiclass_loss: 0.0624 - val_binary_accuracy: 0.9854 - val_binary_loss: 0.0434 - val_loss: 0.1564 - val_multiclass_accuracy: 0.9813 - val_multiclass_loss: 0.0566 - learning_rate: 5.0000e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9846 - binary_loss: 0.0500 - loss: 0.1755 - multiclass_accuracy: 0.9804 - multiclass_loss: 0.0627 - val_binary_accuracy: 0.9854 - val_binary_loss: 0.0447 - val_loss: 0.1587 - val_multiclass_accuracy: 0.9815 - val_multiclass_loss: 0.0574 - learning_rate: 5.0000e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - binary_accuracy: 0.9841 - binary_loss: 0.0502 - loss: 0.1743 - multiclass_accuracy: 0.9805 - multiclass_loss: 0.0620 - val_binary_accuracy: 0.9853 - val_binary_loss: 0.0439 - val_loss: 0.1586 - val_multiclass_accuracy: 0.9811 - val_multiclass_loss: 0.0575 - learning_rate: 5.0000e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m546/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - binary_accuracy: 0.9840 - binary_loss: 0.0506 - loss: 0.1751 - multiclass_accuracy: 0.9803 - multiclass_loss: 0.0623\n",
            "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9840 - binary_loss: 0.0506 - loss: 0.1752 - multiclass_accuracy: 0.9803 - multiclass_loss: 0.0623 - val_binary_accuracy: 0.9857 - val_binary_loss: 0.0436 - val_loss: 0.1564 - val_multiclass_accuracy: 0.9814 - val_multiclass_loss: 0.0564 - learning_rate: 5.0000e-04\n",
            "Epoch 26/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - binary_accuracy: 0.9848 - binary_loss: 0.0485 - loss: 0.1679 - multiclass_accuracy: 0.9816 - multiclass_loss: 0.0597 - val_binary_accuracy: 0.9859 - val_binary_loss: 0.0427 - val_loss: 0.1538 - val_multiclass_accuracy: 0.9822 - val_multiclass_loss: 0.0557 - learning_rate: 2.5000e-04\n",
            "Epoch 27/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - binary_accuracy: 0.9846 - binary_loss: 0.0475 - loss: 0.1633 - multiclass_accuracy: 0.9817 - multiclass_loss: 0.0579 - val_binary_accuracy: 0.9858 - val_binary_loss: 0.0431 - val_loss: 0.1562 - val_multiclass_accuracy: 0.9817 - val_multiclass_loss: 0.0567 - learning_rate: 2.5000e-04\n",
            "Epoch 28/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - binary_accuracy: 0.9851 - binary_loss: 0.0476 - loss: 0.1622 - multiclass_accuracy: 0.9822 - multiclass_loss: 0.0573 - val_binary_accuracy: 0.9845 - val_binary_loss: 0.0459 - val_loss: 0.1652 - val_multiclass_accuracy: 0.9807 - val_multiclass_loss: 0.0598 - learning_rate: 2.5000e-04\n",
            "Epoch 29/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9851 - binary_loss: 0.0478 - loss: 0.1647 - multiclass_accuracy: 0.9817 - multiclass_loss: 0.0584 - val_binary_accuracy: 0.9857 - val_binary_loss: 0.0429 - val_loss: 0.1525 - val_multiclass_accuracy: 0.9822 - val_multiclass_loss: 0.0551 - learning_rate: 2.5000e-04\n",
            "Epoch 30/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9856 - binary_loss: 0.0463 - loss: 0.1577 - multiclass_accuracy: 0.9826 - multiclass_loss: 0.0557 - val_binary_accuracy: 0.9858 - val_binary_loss: 0.0428 - val_loss: 0.1550 - val_multiclass_accuracy: 0.9820 - val_multiclass_loss: 0.0564 - learning_rate: 2.5000e-04\n",
            "Epoch 31/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - binary_accuracy: 0.9853 - binary_loss: 0.0475 - loss: 0.1633 - multiclass_accuracy: 0.9818 - multiclass_loss: 0.0579 - val_binary_accuracy: 0.9846 - val_binary_loss: 0.0472 - val_loss: 0.1693 - val_multiclass_accuracy: 0.9803 - val_multiclass_loss: 0.0612 - learning_rate: 2.5000e-04\n",
            "Epoch 32/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9856 - binary_loss: 0.0477 - loss: 0.1624 - multiclass_accuracy: 0.9826 - multiclass_loss: 0.0573 - val_binary_accuracy: 0.9855 - val_binary_loss: 0.0439 - val_loss: 0.1573 - val_multiclass_accuracy: 0.9819 - val_multiclass_loss: 0.0568 - learning_rate: 2.5000e-04\n",
            "Epoch 33/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9852 - binary_loss: 0.0473 - loss: 0.1602 - multiclass_accuracy: 0.9820 - multiclass_loss: 0.0565 - val_binary_accuracy: 0.9825 - val_binary_loss: 0.0559 - val_loss: 0.1988 - val_multiclass_accuracy: 0.9774 - val_multiclass_loss: 0.0714 - learning_rate: 2.5000e-04\n",
            "Epoch 34/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9854 - binary_loss: 0.0467 - loss: 0.1589 - multiclass_accuracy: 0.9822 - multiclass_loss: 0.0561 - val_binary_accuracy: 0.9827 - val_binary_loss: 0.0551 - val_loss: 0.2009 - val_multiclass_accuracy: 0.9772 - val_multiclass_loss: 0.0729 - learning_rate: 2.5000e-04\n",
            "Epoch 35/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9851 - binary_loss: 0.0468 - loss: 0.1595 - multiclass_accuracy: 0.9822 - multiclass_loss: 0.0563 - val_binary_accuracy: 0.9857 - val_binary_loss: 0.0430 - val_loss: 0.1539 - val_multiclass_accuracy: 0.9820 - val_multiclass_loss: 0.0557 - learning_rate: 2.5000e-04\n",
            "Epoch 36/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - binary_accuracy: 0.9855 - binary_loss: 0.0452 - loss: 0.1518 - multiclass_accuracy: 0.9832 - multiclass_loss: 0.0533 - val_binary_accuracy: 0.9865 - val_binary_loss: 0.0422 - val_loss: 0.1482 - val_multiclass_accuracy: 0.9838 - val_multiclass_loss: 0.0534 - learning_rate: 2.5000e-04\n",
            "Epoch 37/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9853 - binary_loss: 0.0460 - loss: 0.1553 - multiclass_accuracy: 0.9826 - multiclass_loss: 0.0546 - val_binary_accuracy: 0.9853 - val_binary_loss: 0.0444 - val_loss: 0.1587 - val_multiclass_accuracy: 0.9819 - val_multiclass_loss: 0.0574 - learning_rate: 2.5000e-04\n",
            "Epoch 38/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9848 - binary_loss: 0.0477 - loss: 0.1613 - multiclass_accuracy: 0.9819 - multiclass_loss: 0.0568 - val_binary_accuracy: 0.9860 - val_binary_loss: 0.0426 - val_loss: 0.1576 - val_multiclass_accuracy: 0.9817 - val_multiclass_loss: 0.0576 - learning_rate: 2.5000e-04\n",
            "Epoch 39/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9862 - binary_loss: 0.0460 - loss: 0.1557 - multiclass_accuracy: 0.9832 - multiclass_loss: 0.0548 - val_binary_accuracy: 0.9852 - val_binary_loss: 0.0470 - val_loss: 0.1639 - val_multiclass_accuracy: 0.9817 - val_multiclass_loss: 0.0588 - learning_rate: 2.5000e-04\n",
            "Epoch 40/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9860 - binary_loss: 0.0454 - loss: 0.1532 - multiclass_accuracy: 0.9830 - multiclass_loss: 0.0539 - val_binary_accuracy: 0.9851 - val_binary_loss: 0.0443 - val_loss: 0.1569 - val_multiclass_accuracy: 0.9815 - val_multiclass_loss: 0.0564 - learning_rate: 2.5000e-04\n",
            "Epoch 41/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - binary_accuracy: 0.9858 - binary_loss: 0.0452 - loss: 0.1526 - multiclass_accuracy: 0.9834 - multiclass_loss: 0.0537 - val_binary_accuracy: 0.9855 - val_binary_loss: 0.0449 - val_loss: 0.1599 - val_multiclass_accuracy: 0.9821 - val_multiclass_loss: 0.0580 - learning_rate: 2.5000e-04\n",
            "Epoch 42/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9860 - binary_loss: 0.0455 - loss: 0.1529 - multiclass_accuracy: 0.9828 - multiclass_loss: 0.0537 - val_binary_accuracy: 0.9847 - val_binary_loss: 0.0469 - val_loss: 0.1717 - val_multiclass_accuracy: 0.9795 - val_multiclass_loss: 0.0623 - learning_rate: 2.5000e-04\n",
            "Epoch 43/50\n",
            "\u001b[1m544/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - binary_accuracy: 0.9856 - binary_loss: 0.0459 - loss: 0.1544 - multiclass_accuracy: 0.9827 - multiclass_loss: 0.0543\n",
            "Epoch 43: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9856 - binary_loss: 0.0459 - loss: 0.1545 - multiclass_accuracy: 0.9827 - multiclass_loss: 0.0543 - val_binary_accuracy: 0.9860 - val_binary_loss: 0.0427 - val_loss: 0.1528 - val_multiclass_accuracy: 0.9817 - val_multiclass_loss: 0.0552 - learning_rate: 2.5000e-04\n",
            "Epoch 44/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9861 - binary_loss: 0.0455 - loss: 0.1523 - multiclass_accuracy: 0.9833 - multiclass_loss: 0.0534 - val_binary_accuracy: 0.9861 - val_binary_loss: 0.0422 - val_loss: 0.1479 - val_multiclass_accuracy: 0.9830 - val_multiclass_loss: 0.0530 - learning_rate: 1.2500e-04\n",
            "Epoch 45/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9865 - binary_loss: 0.0439 - loss: 0.1468 - multiclass_accuracy: 0.9841 - multiclass_loss: 0.0515 - val_binary_accuracy: 0.9859 - val_binary_loss: 0.0425 - val_loss: 0.1517 - val_multiclass_accuracy: 0.9825 - val_multiclass_loss: 0.0546 - learning_rate: 1.2500e-04\n",
            "Epoch 46/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - binary_accuracy: 0.9860 - binary_loss: 0.0447 - loss: 0.1500 - multiclass_accuracy: 0.9829 - multiclass_loss: 0.0527 - val_binary_accuracy: 0.9853 - val_binary_loss: 0.0446 - val_loss: 0.1548 - val_multiclass_accuracy: 0.9824 - val_multiclass_loss: 0.0553 - learning_rate: 1.2500e-04\n",
            "Epoch 47/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 23ms/step - binary_accuracy: 0.9855 - binary_loss: 0.0449 - loss: 0.1496 - multiclass_accuracy: 0.9833 - multiclass_loss: 0.0524 - val_binary_accuracy: 0.9861 - val_binary_loss: 0.0429 - val_loss: 0.1515 - val_multiclass_accuracy: 0.9828 - val_multiclass_loss: 0.0544 - learning_rate: 1.2500e-04\n",
            "Epoch 48/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9866 - binary_loss: 0.0446 - loss: 0.1490 - multiclass_accuracy: 0.9839 - multiclass_loss: 0.0522 - val_binary_accuracy: 0.9861 - val_binary_loss: 0.0421 - val_loss: 0.1464 - val_multiclass_accuracy: 0.9833 - val_multiclass_loss: 0.0523 - learning_rate: 1.2500e-04\n",
            "Epoch 49/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9860 - binary_loss: 0.0443 - loss: 0.1471 - multiclass_accuracy: 0.9837 - multiclass_loss: 0.0514 - val_binary_accuracy: 0.9861 - val_binary_loss: 0.0424 - val_loss: 0.1477 - val_multiclass_accuracy: 0.9828 - val_multiclass_loss: 0.0527 - learning_rate: 1.2500e-04\n",
            "Epoch 50/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9862 - binary_loss: 0.0440 - loss: 0.1456 - multiclass_accuracy: 0.9840 - multiclass_loss: 0.0508 - val_binary_accuracy: 0.9859 - val_binary_loss: 0.0432 - val_loss: 0.1526 - val_multiclass_accuracy: 0.9823 - val_multiclass_loss: 0.0548 - learning_rate: 1.2500e-04\n",
            "Restoring model weights from the end of the best epoch: 36.\n",
            "\n",
            "[7] Evaluating...\n",
            "\n",
            "======================================================================\n",
            "EVALUATION RESULTS\n",
            "======================================================================\n",
            "\n",
            "Binary Accuracy:  0.9861\n",
            "Binary FPR:       0.0046\n",
            "Multiclass Accuracy: 0.9829\n",
            "Multiclass F1:    0.9820\n",
            "\n",
            "[8] Generating materials...\n",
            "\n",
            "================================================================================\n",
            "GENERATING REPORT MATERIALS\n",
            "================================================================================\n",
            "\n",
            "[1] Generating images...\n",
            "Saved: image1_training_history.png\n",
            "Saved: image2_binary_confusion_matrix.png\n",
            "Saved: image3_roc_curves.png\n",
            "Saved: image4_multiclass_confusion_matrix.png\n",
            "Saved: image5_class_distribution.png\n",
            "\n",
            "[2] Generating tables...\n",
            "\n",
            "Saved: table1_comparison.csv\n",
            "\n",
            "====================================================================================================\n",
            "BASELINE COMPARISON\n",
            "====================================================================================================\n",
            "+------------------------------------+-------------------+--------------------+-----------------+-------------+--------+-----------------------+-----------------+\n",
            "| Method                             |   Binary Accuracy |   Binary Precision |   Binary Recall |   Binary F1 |    FPR | Multiclass Accuracy   | Multiclass F1   |\n",
            "+====================================+===================+====================+=================+=============+========+=======================+=================+\n",
            "| Autoencoder (Miguel-Diez 2025)     |            0.984  |             0.975  |          0.982  |      0.9785 | 0.031  | N/A                   | N/A             |\n",
            "+------------------------------------+-------------------+--------------------+-----------------+-------------+--------+-----------------------+-----------------+\n",
            "| Isolation Forest                   |            0.912  |             0.885  |          0.924  |      0.9041 | 0.089  | N/A                   | N/A             |\n",
            "+------------------------------------+-------------------+--------------------+-----------------+-------------+--------+-----------------------+-----------------+\n",
            "| One-Class SVM                      |            0.875  |             0.832  |          0.901  |      0.8651 | 0.125  | N/A                   | N/A             |\n",
            "+------------------------------------+-------------------+--------------------+-----------------+-------------+--------+-----------------------+-----------------+\n",
            "| LSTM Only                          |            0.956  |             0.942  |          0.958  |      0.95   | 0.045  | 0.9230                | 0.9180          |\n",
            "+------------------------------------+-------------------+--------------------+-----------------+-------------+--------+-----------------------+-----------------+\n",
            "| Proposed Hybrid CNN-LSTM-Attention |            0.9861 |             0.9891 |          0.9642 |      0.9765 | 0.0046 | 0.9829                | 0.9820          |\n",
            "+------------------------------------+-------------------+--------------------+-----------------+-------------+--------+-----------------------+-----------------+\n",
            "\n",
            "Saved: table2_detailed_metrics.csv\n",
            "\n",
            "====================================================================================================\n",
            "PER-CLASS METRICS\n",
            "====================================================================================================\n",
            "+---------+-----------+-------------+----------+------------+---------------+-------+------+------+-------+\n",
            "| Class   |   Support |   Precision |   Recall |   F1-Score |   Specificity |    TP |   FP |   FN |    TN |\n",
            "+=========+===========+=============+==========+============+===============+=======+======+======+=======+\n",
            "| Benign  |     10500 |      0.9846 |   0.9953 |     0.99   |        0.9638 | 10451 |  163 |   49 |  4337 |\n",
            "+---------+-----------+-------------+----------+------------+---------------+-------+------+------+-------+\n",
            "| DoS     |      2250 |      0.9894 |   0.992  |     0.9907 |        0.9981 |  2232 |   24 |   18 | 12726 |\n",
            "+---------+-----------+-------------+----------+------------+---------------+-------+------+------+-------+\n",
            "| Probe   |      1500 |      0.9987 |   1      |     0.9993 |        0.9999 |  1500 |    2 |    0 | 13498 |\n",
            "+---------+-----------+-------------+----------+------------+---------------+-------+------+------+-------+\n",
            "| Exploit |       600 |      0.8801 |   0.685  |     0.7704 |        0.9961 |   411 |   56 |  189 | 14344 |\n",
            "+---------+-----------+-------------+----------+------------+---------------+-------+------+------+-------+\n",
            "| Malware |       150 |      0.9317 |   1      |     0.9646 |        0.9993 |   150 |   11 |    0 | 14839 |\n",
            "+---------+-----------+-------------+----------+------------+---------------+-------+------+------+-------+\n",
            "\n",
            "Saved: table3_architecture.csv\n",
            "\n",
            "================================================================================\n",
            "MODEL ARCHITECTURE\n",
            "================================================================================\n",
            "+----------------------+--------------------------------------------------------+\n",
            "| Component            | Details                                                |\n",
            "+======================+========================================================+\n",
            "| Input Dimension      | 22                                                     |\n",
            "+----------------------+--------------------------------------------------------+\n",
            "| CNN Layers           | Conv1D(64) -> Conv1D(128) with BatchNorm & Dropout     |\n",
            "+----------------------+--------------------------------------------------------+\n",
            "| LSTM Layers          | Bidirectional LSTM(128) with Dropout(0.3)              |\n",
            "+----------------------+--------------------------------------------------------+\n",
            "| Attention Mechanism  | Custom Attention with Dense & Softmax                  |\n",
            "+----------------------+--------------------------------------------------------+\n",
            "| Dense Layers         | Dense(256) -> Dense(128) with BatchNorm & Dropout(0.4) |\n",
            "+----------------------+--------------------------------------------------------+\n",
            "| Output Heads         | Binary + Multiclass (5 classes)                        |\n",
            "+----------------------+--------------------------------------------------------+\n",
            "| Total Parameters     | 390,151                                                |\n",
            "+----------------------+--------------------------------------------------------+\n",
            "| Trainable Parameters | 388,999                                                |\n",
            "+----------------------+--------------------------------------------------------+\n",
            "| Optimizer            | Adam                                                   |\n",
            "+----------------------+--------------------------------------------------------+\n",
            "| Learning Rate        | 0.0005                                                 |\n",
            "+----------------------+--------------------------------------------------------+\n",
            "\n",
            "================================================================================\n",
            "COMPLETE\n",
            "================================================================================\n",
            "\n",
            "======================================================================\n",
            "SUCCESS\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}