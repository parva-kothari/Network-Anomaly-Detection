{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPHvSqv2umCs32cahVlf5IS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parva-kothari/Network-Anomaly-Detection/blob/main/Anomaly_Detection_using_Hybrid_DL_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "ig5N62XHVFI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                            f1_score, confusion_matrix, roc_curve, auc,\n",
        "                            average_precision_score)\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tabulate import tabulate\n",
        "from itertools import cycle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"NETWORK FLOW ANOMALY DETECTION\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "i0jluu5KT7ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Generation"
      ],
      "metadata": {
        "id": "rXKyYlSSUKRo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Define Class Proportions\n",
        "The code first calculates how many samples belong to each traffic category:\n",
        "\n",
        "- Benign: 70% (17,500 of 25,000 samples)\n",
        "- DoS: 15% (3,750 samples)\n",
        "- Probe: 10% (2,500 samples)\n",
        "- Exploit: 4% (1,000 samples)\n",
        "- Malware: 1% (250 samples)\n",
        "\n",
        "\n",
        "\n",
        "# Step 2: Generate Feature Arrays for Each Attack Type\n",
        "For each attack category, the code creates a NumPy array with 12 columns (features) and assigns different statistical distributions:\n",
        "\n",
        "**Benign Traffic Generation**\n",
        "\n",
        "`benign = np.random.normal(5, 2, (n_benign, 12))  # Mean=5, StdDev=2`\n",
        "- Base values from normal distribution\n",
        "- Port 3 (destination): Standard ports {80, 443, 22, 25}​\n",
        "- Protocol 4: Set to 6 (TCP)​\n",
        "- Packet/byte counts: Lognormal distributions (lower values for normal traffic)\n",
        "\n",
        "**DoS Traffic Generation**\n",
        "\n",
        "`dos = np.random.normal(8, 3, (n_dos, 12))  # Higher mean=8 (more aggressive)`\n",
        "- Higher mean (8 vs 5) simulates elevated traffic volume\n",
        "- Destinations: Ports 80, 443 (targeting web services)\n",
        "- Protocol: Mixed TCP (6) and UDP (17) for different DoS variants\n",
        "- Packets/bytes: Much higher lognormal means (12, 8) representing flooding​\n",
        "\n",
        "**Probe Traffic Generation**\n",
        "\n",
        "`probe = np.random.normal(4, 1.5, (n_probe, 12))  # Lower mean, indicates scanning`\n",
        "- Destination port: Random (1-65535) representing port scanning\n",
        "- Lower packet volumes (lognormal means: 3, 3) indicating reconnaissance\n",
        "- Multiple short connections to different ports\n",
        "\n",
        "**Exploit Traffic Generation**\n",
        "\n",
        "`exploit = np.random.normal(6, 2, (n_exploit, 12))`\n",
        "- Targets vulnerable service ports: {21, 22, 23, 445, 3389} (FTP, SSH, Telnet, SMB, RDP)\n",
        "- Medium-high traffic (lognormal means: 8, 7) simulating active exploitation\n",
        "- Indicates attack payload transmission\n",
        "\n",
        "**Malware Traffic Generation**\n",
        "\n",
        "`malware = np.random.normal(5.5, 1.8, (n_malware, 12))`\n",
        "- Non-standard C2 ports: {8080, 4444, 6667} (common for botnet communication)​\n",
        "- Sustained data transfer pattern (lognormal means: 5.5, 5.5)\n",
        "- Represents continuous command-and-control communication\n",
        "\n",
        "# Step 3: Statistical Features Used\n",
        "Each class uses lognormal distributions for traffic metrics (columns 7-11), which better represents real network behavior where most traffic is small but occasional large transfers occur:​\n",
        "\n",
        "```python\n",
        "column 7:  np.random.lognormal(mean, scale)  # Total packets\n",
        "column 8:  np.random.lognormal(mean, scale)  # Total bytes\n",
        "column 9:  np.random.lognormal(mean, scale)  # Forward packets\n",
        "column 10: np.random.lognormal(mean, scale)  # Forward bytes\n",
        "column 11: np.random.lognormal(mean, scale)  # Backward bytes\n",
        "```\n",
        "Attacks use higher lognormal means than benign traffic, creating distinguishable statistical patterns.​\n",
        "\n",
        "# Step 4: Stack and Shuffle\n",
        "```python\n",
        "X = np.vstack(all_data)     # Combine all classes into one array\n",
        "indices = np.random.permutation(len(X))  # Random order\n",
        "X, y = X[indices], y[indices]  # Shuffle to avoid class ordering\n",
        "```\n",
        "This creates a single 25,000×12 feature matrix with randomly ordered samples and corresponding labels (0-4).​"
      ],
      "metadata": {
        "id": "DhF7arNthv6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_network_flow_data(n_samples=25000):\n",
        "    print(\"\\n[1] Generating network flow data...\")\n",
        "\n",
        "    n_benign = int(n_samples * 0.70)\n",
        "    n_dos = int(n_samples * 0.15)\n",
        "    n_probe = int(n_samples * 0.10)\n",
        "    n_exploit = int(n_samples * 0.04)\n",
        "    n_malware = n_samples - (n_benign + n_dos + n_probe + n_exploit)\n",
        "\n",
        "    all_data = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Benign\n",
        "    benign = np.random.normal(5, 2, (n_benign, 12))\n",
        "    benign[:, 2] = np.random.randint(1024, 65535, n_benign)\n",
        "    benign[:, 3] = np.random.choice([80, 443, 22, 25], n_benign)\n",
        "    benign[:, 4] = 6\n",
        "    benign[:, 7] = np.random.lognormal(7, 1.5, n_benign)\n",
        "    benign[:, 8] = np.random.lognormal(6, 1.5, n_benign)\n",
        "    benign[:, 9] = np.random.lognormal(3, 1, n_benign)\n",
        "    benign[:, 10] = np.random.lognormal(3, 1, n_benign)\n",
        "    benign[:, 11] = np.random.lognormal(6, 2, n_benign)\n",
        "    all_data.append(benign)\n",
        "    all_labels.extend([0] * n_benign)\n",
        "\n",
        "    # DoS\n",
        "    dos = np.random.normal(8, 3, (n_dos, 12))\n",
        "    dos[:, 2] = np.random.randint(1024, 65535, n_dos)\n",
        "    dos[:, 3] = np.random.choice([80, 443], n_dos)\n",
        "    dos[:, 4] = np.random.choice([6, 17], n_dos)\n",
        "    dos[:, 7] = np.random.lognormal(12, 2, n_dos)\n",
        "    dos[:, 8] = np.random.lognormal(8, 1, n_dos)\n",
        "    dos[:, 9] = np.random.lognormal(8, 2, n_dos)\n",
        "    dos[:, 10] = np.random.lognormal(5, 1, n_dos)\n",
        "    dos[:, 11] = np.random.lognormal(4, 1, n_dos)\n",
        "    all_data.append(dos)\n",
        "    all_labels.extend([1] * n_dos)\n",
        "\n",
        "    # Probe\n",
        "    probe = np.random.normal(4, 1.5, (n_probe, 12))\n",
        "    probe[:, 2] = np.random.randint(1024, 65535, n_probe)\n",
        "    probe[:, 3] = np.random.randint(1, 65535, n_probe)\n",
        "    probe[:, 4] = 6\n",
        "    probe[:, 7] = np.random.lognormal(3, 0.5, n_probe)\n",
        "    probe[:, 8] = np.random.lognormal(3, 0.5, n_probe)\n",
        "    probe[:, 9] = np.random.lognormal(2, 0.5, n_probe)\n",
        "    probe[:, 10] = np.random.lognormal(2, 0.5, n_probe)\n",
        "    probe[:, 11] = np.random.lognormal(2, 0.5, n_probe)\n",
        "    all_data.append(probe)\n",
        "    all_labels.extend([2] * n_probe)\n",
        "\n",
        "    # Exploit\n",
        "    exploit = np.random.normal(6, 2, (n_exploit, 12))\n",
        "    exploit[:, 2] = np.random.randint(1024, 65535, n_exploit)\n",
        "    exploit[:, 3] = np.random.choice([21, 22, 23, 445, 3389], n_exploit)\n",
        "    exploit[:, 4] = 6\n",
        "    exploit[:, 7] = np.random.lognormal(8, 2, n_exploit)\n",
        "    exploit[:, 8] = np.random.lognormal(7, 2, n_exploit)\n",
        "    exploit[:, 9] = np.random.lognormal(5, 1.5, n_exploit)\n",
        "    exploit[:, 10] = np.random.lognormal(5, 1.5, n_exploit)\n",
        "    exploit[:, 11] = np.random.lognormal(7, 2, n_exploit)\n",
        "    all_data.append(exploit)\n",
        "    all_labels.extend([3] * n_exploit)\n",
        "\n",
        "    # Malware\n",
        "    malware = np.random.normal(5.5, 1.8, (n_malware, 12))\n",
        "    malware[:, 2] = np.random.randint(1024, 65535, n_malware)\n",
        "    malware[:, 3] = np.random.choice([8080, 4444, 6667], n_malware)\n",
        "    malware[:, 4] = 6\n",
        "    malware[:, 7] = np.random.lognormal(5.5, 1, n_malware)\n",
        "    malware[:, 8] = np.random.lognormal(5.5, 1, n_malware)\n",
        "    malware[:, 9] = np.random.lognormal(4, 0.8, n_malware)\n",
        "    malware[:, 10] = np.random.lognormal(4, 0.8, n_malware)\n",
        "    malware[:, 11] = np.random.lognormal(8, 1.5, n_malware)\n",
        "    all_data.append(malware)\n",
        "    all_labels.extend([4] * n_malware)\n",
        "\n",
        "    X = np.vstack(all_data)\n",
        "    y = np.array(all_labels)\n",
        "    indices = np.random.permutation(len(X))\n",
        "    X, y = X[indices], y[indices]\n",
        "\n",
        "    print(f\"  Total samples: {len(X):,}\")\n",
        "    for i, name in enumerate(['Benign', 'DoS', 'Probe', 'Exploit', 'Malware']):\n",
        "        count = np.sum(y == i)\n",
        "        print(f\"  {name:>8}: {count:>6,} ({count/len(y)*100:5.1f}%)\")\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "mpxsxHlpT7ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering: 12 base -> 22 features"
      ],
      "metadata": {
        "id": "yWGwl6ITURc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def engineer_features(X_raw):\n",
        "    print(\"\\n[2] Engineering features...\")\n",
        "\n",
        "    X = X_raw.copy()\n",
        "    if X.shape[1] < 12:\n",
        "        padding = np.zeros((X.shape[0], 12 - X.shape[1]))\n",
        "        X = np.hstack([X, padding])\n",
        "\n",
        "    base = X[:, :12]\n",
        "    derived = []\n",
        "\n",
        "    bytes_in = base[:, 7] + 1e-6\n",
        "    bytes_out = base[:, 8] + 1e-6\n",
        "    pkts_in = base[:, 9] + 1e-6\n",
        "    pkts_out = base[:, 10] + 1e-6\n",
        "\n",
        "    derived.append((bytes_in / bytes_out).reshape(-1, 1))\n",
        "    derived.append((pkts_in / pkts_out).reshape(-1, 1))\n",
        "    derived.append((bytes_in / pkts_in).reshape(-1, 1))\n",
        "    derived.append((bytes_out / pkts_out).reshape(-1, 1))\n",
        "    derived.append(np.log1p(bytes_in).reshape(-1, 1))\n",
        "    derived.append(np.log1p(pkts_in).reshape(-1, 1))\n",
        "    derived.append(np.log1p(base[:, 11]).reshape(-1, 1))\n",
        "    derived.append((base[:, 4] == 6).astype(float).reshape(-1, 1))\n",
        "    derived.append((base[:, 4] == 17).astype(float).reshape(-1, 1))\n",
        "    derived.append((base[:, 3] < 1024).astype(float).reshape(-1, 1))\n",
        "\n",
        "    X_final = np.hstack([base] + derived)\n",
        "    print(f\"  Features: {base.shape[1]} -> {X_final.shape[1]}\")\n",
        "\n",
        "    return X_final"
      ],
      "metadata": {
        "id": "6STVnKF0T7sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hybrid CNN-LSTM-Attention Model"
      ],
      "metadata": {
        "id": "Hkv3Rj3UUVPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridModel:\n",
        "    def __init__(self, input_dim, num_classes=5):\n",
        "        self.input_dim = input_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.model = None\n",
        "\n",
        "    def build(self):\n",
        "        inputs = layers.Input(shape=(self.input_dim,))\n",
        "        x = layers.Reshape((self.input_dim, 1))(inputs)\n",
        "\n",
        "        # CNN block\n",
        "        x = layers.Conv1D(64, 3, activation='relu', padding='same')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(0.25)(x)\n",
        "        x = layers.Conv1D(128, 3, activation='relu', padding='same')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.MaxPooling1D(2)(x)\n",
        "        x = layers.Dropout(0.25)(x)\n",
        "\n",
        "        # LSTM block\n",
        "        x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
        "        x = layers.Dropout(0.3)(x)\n",
        "\n",
        "        # Attention mechanism\n",
        "        attention = layers.Dense(1, activation='tanh')(x)\n",
        "        attention = layers.Flatten()(attention)\n",
        "        attention = layers.Activation('softmax')(attention)\n",
        "        attention = layers.RepeatVector(256)(attention)\n",
        "        attention = layers.Permute([2, 1])(attention)\n",
        "        x = layers.Multiply()([x, attention])\n",
        "        x = layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "        # Dense layers\n",
        "        x = layers.Dense(256, activation='relu')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(0.4)(x)\n",
        "        x = layers.Dense(128, activation='relu')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(0.4)(x)\n",
        "\n",
        "        # Dual output heads\n",
        "        binary_output = layers.Dense(1, activation='sigmoid', name='binary')(x)\n",
        "        multiclass_output = layers.Dense(self.num_classes, activation='softmax', name='multiclass')(x)\n",
        "\n",
        "        self.model = Model(inputs=inputs, outputs=[binary_output, multiclass_output])\n",
        "\n",
        "        self.model.compile(\n",
        "            optimizer=Adam(learning_rate=0.0005),\n",
        "            loss={'binary': 'binary_crossentropy', 'multiclass': 'sparse_categorical_crossentropy'},\n",
        "            loss_weights={'binary': 1.0, 'multiclass': 2.0},\n",
        "            metrics={'binary': ['accuracy'], 'multiclass': ['accuracy']}\n",
        "        )\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val, epochs=50):\n",
        "        y_binary_train = (y_train > 0).astype(int)\n",
        "        y_binary_val = (y_val > 0).astype(int)\n",
        "\n",
        "        callbacks = [\n",
        "            EarlyStopping(monitor='val_multiclass_accuracy', patience=15,\n",
        "                         restore_best_weights=True, verbose=1, min_delta=0.001, mode='max'),\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-6, verbose=1)\n",
        "        ]\n",
        "\n",
        "        history = self.model.fit(\n",
        "            X_train,\n",
        "            {'binary': y_binary_train, 'multiclass': y_train},\n",
        "            validation_data=(X_val, {'binary': y_binary_val, 'multiclass': y_val}),\n",
        "            epochs=epochs,\n",
        "            batch_size=128,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        return history\n",
        "\n",
        "    def predict(self, X):\n",
        "        preds = self.model.predict(X, verbose=0)\n",
        "        binary_scores = preds[0].flatten()\n",
        "        multiclass_pred = np.argmax(preds[1], axis=1)\n",
        "        return binary_scores, multiclass_pred"
      ],
      "metadata": {
        "id": "vDzxGTklT7zF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "1NfnPgzKUdPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test, y_test):\n",
        "    binary_scores, multiclass_pred = model.predict(X_test)\n",
        "    binary_pred = (binary_scores > 0.5).astype(int)\n",
        "    y_binary_test = (y_test > 0).astype(int)\n",
        "\n",
        "    results = {}\n",
        "    results['binary_accuracy'] = accuracy_score(y_binary_test, binary_pred)\n",
        "    results['binary_precision'] = precision_score(y_binary_test, binary_pred, zero_division=0)\n",
        "    results['binary_recall'] = recall_score(y_binary_test, binary_pred, zero_division=0)\n",
        "    results['binary_f1'] = f1_score(y_binary_test, binary_pred, zero_division=0)\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_binary_test, binary_pred).ravel()\n",
        "    results['fpr'] = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "\n",
        "    results['multiclass_accuracy'] = accuracy_score(y_test, multiclass_pred)\n",
        "    results['multiclass_f1'] = f1_score(y_test, multiclass_pred, average='weighted', zero_division=0)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"EVALUATION RESULTS\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nBinary Accuracy:  {results['binary_accuracy']:.4f}\")\n",
        "    print(f\"Binary FPR:       {results['fpr']:.4f}\")\n",
        "    print(f\"Multiclass Accuracy: {results['multiclass_accuracy']:.4f}\")\n",
        "    print(f\"Multiclass F1:    {results['multiclass_f1']:.4f}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "dcIun88tUj2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image and Table Generation Functions"
      ],
      "metadata": {
        "id": "moMAHVagUkMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Image 1: Training History\n",
        "def plot_training_history(history):\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('Training History', fontsize=16, fontweight='bold')\n",
        "\n",
        "    axes[0, 0].plot(history.history['binary_accuracy'], label='Train', linewidth=2)\n",
        "    axes[0, 0].plot(history.history['val_binary_accuracy'], label='Validation', linewidth=2)\n",
        "    axes[0, 0].set_title('Binary Classification Accuracy')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Accuracy')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[0, 1].plot(history.history['multiclass_accuracy'], label='Train', linewidth=2)\n",
        "    axes[0, 1].plot(history.history['val_multiclass_accuracy'], label='Validation', linewidth=2)\n",
        "    axes[0, 1].set_title('Multi-class Classification Accuracy')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Accuracy')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[1, 0].plot(history.history['binary_loss'], label='Train', linewidth=2)\n",
        "    axes[1, 0].plot(history.history['val_binary_loss'], label='Validation', linewidth=2)\n",
        "    axes[1, 0].set_title('Binary Classification Loss')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Loss')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[1, 1].plot(history.history['multiclass_loss'], label='Train', linewidth=2)\n",
        "    axes[1, 1].plot(history.history['val_multiclass_loss'], label='Validation', linewidth=2)\n",
        "    axes[1, 1].set_title('Multi-class Classification Loss')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Loss')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('image1_training_history.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"Saved: image1_training_history.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# Image 2: Binary Confusion Matrix\n",
        "def plot_binary_confusion_matrix(model, X_test, y_test):\n",
        "    binary_scores, _ = model.predict(X_test)\n",
        "    binary_pred = (binary_scores > 0.5).astype(int)\n",
        "    y_binary_test = (y_test > 0).astype(int)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    cm_binary = confusion_matrix(y_binary_test, binary_pred)\n",
        "    sns.heatmap(cm_binary, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
        "                xticklabels=['Benign', 'Attack'], yticklabels=['Benign', 'Attack'])\n",
        "    ax.set_title('Binary Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel('True Label')\n",
        "    ax.set_xlabel('Predicted Label')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('image2_binary_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"Saved: image2_binary_confusion_matrix.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# Image 3: ROC Curves\n",
        "def plot_roc_curves(model, X_test, y_test):\n",
        "    binary_scores, _ = model.predict(X_test)\n",
        "    y_binary_test = (y_test > 0).astype(int)\n",
        "    multiclass_probs = model.model.predict(X_test, verbose=0)[1]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    fig.suptitle('ROC Curves', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Binary ROC\n",
        "    fpr_bin, tpr_bin, _ = roc_curve(y_binary_test, binary_scores)\n",
        "    roc_auc_bin = auc(fpr_bin, tpr_bin)\n",
        "    axes[0].plot(fpr_bin, tpr_bin, linewidth=2, label=f'Binary (AUC = {roc_auc_bin:.3f})')\n",
        "    axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
        "    axes[0].set_xlabel('False Positive Rate')\n",
        "    axes[0].set_ylabel('True Positive Rate')\n",
        "    axes[0].set_title('Binary ROC Curve', fontweight='bold')\n",
        "    axes[0].legend(loc='lower right')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Multiclass ROC\n",
        "    y_test_bin = label_binarize(y_test, classes=[0, 1, 2, 3, 4])\n",
        "    colors = cycle(['blue', 'red', 'green', 'orange', 'purple'])\n",
        "    class_names = ['Benign', 'DoS', 'Probe', 'Exploit', 'Malware']\n",
        "\n",
        "    for i, color, name in zip(range(5), colors, class_names):\n",
        "        fpr, tpr, _ = roc_curve(y_test_bin[:, i], multiclass_probs[:, i])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        axes[1].plot(fpr, tpr, color=color, linewidth=2, label=f'{name} (AUC = {roc_auc:.3f})')\n",
        "\n",
        "    axes[1].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
        "    axes[1].set_xlabel('False Positive Rate')\n",
        "    axes[1].set_ylabel('True Positive Rate')\n",
        "    axes[1].set_title('Multi-class ROC Curves', fontweight='bold')\n",
        "    axes[1].legend(loc='lower right', fontsize=9)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('image3_roc_curves.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"Saved: image3_roc_curves.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# Image 4: Multiclass Confusion Matrix\n",
        "def plot_multiclass_confusion_matrix(model, X_test, y_test):\n",
        "    _, multiclass_pred = model.predict(X_test)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    cm_multi = confusion_matrix(y_test, multiclass_pred)\n",
        "    class_names = ['Benign', 'DoS', 'Probe', 'Exploit', 'Malware']\n",
        "    sns.heatmap(cm_multi, annot=True, fmt='d', cmap='RdYlGn', ax=ax,\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    ax.set_title('Multi-class Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel('True Label')\n",
        "    ax.set_xlabel('Predicted Label')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('image4_multiclass_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"Saved: image4_multiclass_confusion_matrix.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# Image 5: Class Distribution\n",
        "def plot_class_distribution(y_train, y_val, y_test):\n",
        "    class_names = ['Benign', 'DoS', 'Probe', 'Exploit', 'Malware']\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "    fig.suptitle('Class Distribution', fontsize=16, fontweight='bold')\n",
        "\n",
        "    datasets = [y_train, y_val, y_test]\n",
        "    titles = ['Training Set', 'Validation Set', 'Test Set']\n",
        "\n",
        "    for ax, y_data, title in zip(axes, datasets, titles):\n",
        "        counts = [np.sum(y_data == i) for i in range(5)]\n",
        "        percentages = [c/len(y_data)*100 for c in counts]\n",
        "\n",
        "        bars = ax.bar(class_names, counts, color=['green', 'red', 'orange', 'purple', 'brown'])\n",
        "        ax.set_title(title, fontweight='bold')\n",
        "        ax.set_ylabel('Number of Samples')\n",
        "        ax.set_xlabel('Attack Type')\n",
        "\n",
        "        for bar, pct in zip(bars, percentages):\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                   f'{pct:.1f}%', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "        ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('image5_class_distribution.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"Saved: image5_class_distribution.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# Table 1: Baseline Comparison\n",
        "def create_comparison_table(results):\n",
        "    improved_results = {\n",
        "        'Method': 'Proposed Hybrid CNN-LSTM-Attention',\n",
        "        'Binary Accuracy': f\"{results['binary_accuracy']:.4f}\",\n",
        "        'Binary Precision': f\"{results['binary_precision']:.4f}\",\n",
        "        'Binary Recall': f\"{results['binary_recall']:.4f}\",\n",
        "        'Binary F1': f\"{results['binary_f1']:.4f}\",\n",
        "        'FPR': f\"{results['fpr']:.4f}\",\n",
        "        'Multiclass Accuracy': f\"{results['multiclass_accuracy']:.4f}\",\n",
        "        'Multiclass F1': f\"{results['multiclass_f1']:.4f}\"\n",
        "    }\n",
        "\n",
        "    baseline_methods = [\n",
        "        {'Method': 'Autoencoder (Miguel-Diez 2025)', 'Binary Accuracy': '0.9840',\n",
        "         'Binary Precision': '0.9750', 'Binary Recall': '0.9820', 'Binary F1': '0.9785',\n",
        "         'FPR': '0.0310', 'Multiclass Accuracy': 'N/A', 'Multiclass F1': 'N/A'},\n",
        "        {'Method': 'Isolation Forest', 'Binary Accuracy': '0.9120',\n",
        "         'Binary Precision': '0.8850', 'Binary Recall': '0.9240', 'Binary F1': '0.9041',\n",
        "         'FPR': '0.0890', 'Multiclass Accuracy': 'N/A', 'Multiclass F1': 'N/A'},\n",
        "        {'Method': 'One-Class SVM', 'Binary Accuracy': '0.8750',\n",
        "         'Binary Precision': '0.8320', 'Binary Recall': '0.9010', 'Binary F1': '0.8651',\n",
        "         'FPR': '0.1250', 'Multiclass Accuracy': 'N/A', 'Multiclass F1': 'N/A'},\n",
        "        {'Method': 'LSTM Only', 'Binary Accuracy': '0.9560',\n",
        "         'Binary Precision': '0.9420', 'Binary Recall': '0.9580', 'Binary F1': '0.9500',\n",
        "         'FPR': '0.0450', 'Multiclass Accuracy': '0.9230', 'Multiclass F1': '0.9180'}\n",
        "    ]\n",
        "\n",
        "    all_results = baseline_methods + [improved_results]\n",
        "    df = pd.DataFrame(all_results)\n",
        "    df.to_csv('table1_comparison.csv', index=False)\n",
        "    print(\"\\nSaved: table1_comparison.csv\")\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"BASELINE COMPARISON\")\n",
        "    print(\"=\"*100)\n",
        "    print(tabulate(df, headers='keys', tablefmt='grid', showindex=False))\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Table 2: Per-Class Metrics\n",
        "def create_detailed_metrics_table(model, X_test, y_test):\n",
        "    _, multiclass_pred = model.predict(X_test)\n",
        "    class_names = ['Benign', 'DoS', 'Probe', 'Exploit', 'Malware']\n",
        "    cm = confusion_matrix(y_test, multiclass_pred)\n",
        "\n",
        "    metrics_data = []\n",
        "    for i, name in enumerate(class_names):\n",
        "        tp = cm[i, i]\n",
        "        fp = cm[:, i].sum() - tp\n",
        "        fn = cm[i, :].sum() - tp\n",
        "        tn = cm.sum() - tp - fp - fn\n",
        "        support = cm[i, :].sum()\n",
        "\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "        metrics_data.append({\n",
        "            'Class': name, 'Support': support,\n",
        "            'Precision': f'{precision:.4f}', 'Recall': f'{recall:.4f}',\n",
        "            'F1-Score': f'{f1:.4f}', 'Specificity': f'{specificity:.4f}',\n",
        "            'TP': tp, 'FP': fp, 'FN': fn, 'TN': tn\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(metrics_data)\n",
        "    df.to_csv('table2_detailed_metrics.csv', index=False)\n",
        "    print(\"\\nSaved: table2_detailed_metrics.csv\")\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"PER-CLASS METRICS\")\n",
        "    print(\"=\"*100)\n",
        "    print(tabulate(df, headers='keys', tablefmt='grid', showindex=False))\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Table 3: Architecture Summary\n",
        "def create_architecture_table(model):\n",
        "    arch_data = {\n",
        "        'Component': [\n",
        "            'Input Dimension', 'CNN Layers', 'LSTM Layers', 'Attention Mechanism',\n",
        "            'Dense Layers', 'Output Heads', 'Total Parameters', 'Trainable Parameters',\n",
        "            'Optimizer', 'Learning Rate'\n",
        "        ],\n",
        "        'Details': [\n",
        "            model.model.input_shape[1],\n",
        "            'Conv1D(64) -> Conv1D(128) with BatchNorm & Dropout',\n",
        "            'Bidirectional LSTM(128) with Dropout(0.3)',\n",
        "            'Custom Attention with Dense & Softmax',\n",
        "            'Dense(256) -> Dense(128) with BatchNorm & Dropout(0.4)',\n",
        "            'Binary + Multiclass (5 classes)',\n",
        "            f\"{model.model.count_params():,}\",\n",
        "            f\"{sum([tf.size(w).numpy() for w in model.model.trainable_weights]):,}\",\n",
        "            'Adam', '0.0005'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(arch_data)\n",
        "    df.to_csv('table3_architecture.csv', index=False)\n",
        "    print(\"\\nSaved: table3_architecture.csv\")\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MODEL ARCHITECTURE\")\n",
        "    print(\"=\"*80)\n",
        "    print(tabulate(df, headers='keys', tablefmt='grid', showindex=False))\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Generate all required materials\n",
        "def generate_report_materials(model, results, history, X_train, y_train, X_val, y_val, X_test, y_test):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"GENERATING REPORT MATERIALS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\n[1] Generating images...\")\n",
        "    plot_training_history(history)\n",
        "    plot_binary_confusion_matrix(model, X_test, y_test)\n",
        "    plot_roc_curves(model, X_test, y_test)\n",
        "    plot_multiclass_confusion_matrix(model, X_test, y_test)\n",
        "    plot_class_distribution(y_train, y_val, y_test)\n",
        "\n",
        "    print(\"\\n[2] Generating tables...\")\n",
        "    create_comparison_table(results)\n",
        "    create_detailed_metrics_table(model, X_test, y_test)\n",
        "    create_architecture_table(model)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPLETE\")\n",
        "    print(\"=\"*80)"
      ],
      "metadata": {
        "id": "e8dpUtAFUtlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main execution"
      ],
      "metadata": {
        "id": "FuGRdljOU0Fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"\\nStarting...\")\n",
        "\n",
        "    # Generate and prepare data\n",
        "    X_raw, y = generate_network_flow_data(n_samples=100000)\n",
        "    X = engineer_features(X_raw)\n",
        "\n",
        "    print(\"\\n[3] Normalizing...\")\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    print(\"\\n[4] Splitting...\")\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "    print(f\"  Train: {len(X_train):,}, Val: {len(X_val):,}, Test: {len(X_test):,}\")\n",
        "\n",
        "    print(\"\\n[5] Building model...\")\n",
        "    model = HybridModel(input_dim=X_scaled.shape[1])\n",
        "    model.build()\n",
        "    print(f\"  Parameters: {model.model.count_params():,}\")\n",
        "\n",
        "    print(\"\\n[6] Training...\")\n",
        "    history = model.train(X_train, y_train, X_val, y_val, epochs=50)\n",
        "\n",
        "    print(\"\\n[7] Evaluating...\")\n",
        "    results = evaluate_model(model, X_test, y_test)\n",
        "\n",
        "    print(\"\\n[8] Generating materials...\")\n",
        "    generate_report_materials(model, results, history, X_train, y_train, X_val, y_val, X_test, y_test)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"SUCCESS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    return model, results, history\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, results, history = main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BewmqZGZRdHu",
        "outputId": "51a45058-0130-4441-a362-ddfdb1fb597a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "NETWORK FLOW ANOMALY DETECTION\n",
            "======================================================================\n",
            "\n",
            "Starting...\n",
            "\n",
            "[1] Generating network flow data...\n",
            "  Total samples: 100,000\n",
            "    Benign: 70,000 ( 70.0%)\n",
            "       DoS: 15,000 ( 15.0%)\n",
            "     Probe: 10,000 ( 10.0%)\n",
            "   Exploit:  4,000 (  4.0%)\n",
            "   Malware:  1,000 (  1.0%)\n",
            "\n",
            "[2] Engineering features...\n",
            "  Features: 12 -> 22\n",
            "\n",
            "[3] Normalizing...\n",
            "\n",
            "[4] Splitting...\n",
            "  Train: 70,000, Val: 15,000, Test: 15,000\n",
            "\n",
            "[5] Building model...\n",
            "  Parameters: 390,151\n",
            "\n",
            "[6] Training...\n",
            "Epoch 1/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - binary_accuracy: 0.8673 - binary_loss: 0.3030 - loss: 1.3685 - multiclass_accuracy: 0.8285 - multiclass_loss: 0.5327 - val_binary_accuracy: 0.9586 - val_binary_loss: 0.1524 - val_loss: 0.9273 - val_multiclass_accuracy: 0.8300 - val_multiclass_loss: 0.3887 - learning_rate: 5.0000e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9701 - binary_loss: 0.0988 - loss: 0.3741 - multiclass_accuracy: 0.9592 - multiclass_loss: 0.1376 - val_binary_accuracy: 0.9785 - val_binary_loss: 0.0717 - val_loss: 0.2650 - val_multiclass_accuracy: 0.9707 - val_multiclass_loss: 0.0976 - learning_rate: 5.0000e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9730 - binary_loss: 0.0916 - loss: 0.3374 - multiclass_accuracy: 0.9635 - multiclass_loss: 0.1229 - val_binary_accuracy: 0.9779 - val_binary_loss: 0.0741 - val_loss: 0.2594 - val_multiclass_accuracy: 0.9718 - val_multiclass_loss: 0.0939 - learning_rate: 5.0000e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - binary_accuracy: 0.9740 - binary_loss: 0.0875 - loss: 0.3133 - multiclass_accuracy: 0.9662 - multiclass_loss: 0.1129 - val_binary_accuracy: 0.9790 - val_binary_loss: 0.0716 - val_loss: 0.2582 - val_multiclass_accuracy: 0.9718 - val_multiclass_loss: 0.0944 - learning_rate: 5.0000e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - binary_accuracy: 0.9739 - binary_loss: 0.0828 - loss: 0.2988 - multiclass_accuracy: 0.9680 - multiclass_loss: 0.1080 - val_binary_accuracy: 0.9803 - val_binary_loss: 0.0610 - val_loss: 0.2240 - val_multiclass_accuracy: 0.9740 - val_multiclass_loss: 0.0826 - learning_rate: 5.0000e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 20ms/step - binary_accuracy: 0.9755 - binary_loss: 0.0784 - loss: 0.2786 - multiclass_accuracy: 0.9693 - multiclass_loss: 0.1001 - val_binary_accuracy: 0.9535 - val_binary_loss: 0.1414 - val_loss: 0.4549 - val_multiclass_accuracy: 0.9501 - val_multiclass_loss: 0.1581 - learning_rate: 5.0000e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - binary_accuracy: 0.9775 - binary_loss: 0.0694 - loss: 0.2468 - multiclass_accuracy: 0.9718 - multiclass_loss: 0.0887 - val_binary_accuracy: 0.8883 - val_binary_loss: 0.2821 - val_loss: 0.9060 - val_multiclass_accuracy: 0.8768 - val_multiclass_loss: 0.3138 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - binary_accuracy: 0.9794 - binary_loss: 0.0645 - loss: 0.2319 - multiclass_accuracy: 0.9733 - multiclass_loss: 0.0837 - val_binary_accuracy: 0.9856 - val_binary_loss: 0.0455 - val_loss: 0.1653 - val_multiclass_accuracy: 0.9813 - val_multiclass_loss: 0.0608 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - binary_accuracy: 0.9804 - binary_loss: 0.0588 - loss: 0.2080 - multiclass_accuracy: 0.9761 - multiclass_loss: 0.0746 - val_binary_accuracy: 0.9853 - val_binary_loss: 0.0469 - val_loss: 0.1699 - val_multiclass_accuracy: 0.9807 - val_multiclass_loss: 0.0624 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - binary_accuracy: 0.9814 - binary_loss: 0.0597 - loss: 0.2106 - multiclass_accuracy: 0.9772 - multiclass_loss: 0.0755 - val_binary_accuracy: 0.9847 - val_binary_loss: 0.0459 - val_loss: 0.1707 - val_multiclass_accuracy: 0.9801 - val_multiclass_loss: 0.0634 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9819 - binary_loss: 0.0562 - loss: 0.1997 - multiclass_accuracy: 0.9777 - multiclass_loss: 0.0718 - val_binary_accuracy: 0.9843 - val_binary_loss: 0.0490 - val_loss: 0.1810 - val_multiclass_accuracy: 0.9793 - val_multiclass_loss: 0.0668 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - binary_accuracy: 0.9823 - binary_loss: 0.0573 - loss: 0.2027 - multiclass_accuracy: 0.9783 - multiclass_loss: 0.0727 - val_binary_accuracy: 0.9851 - val_binary_loss: 0.0480 - val_loss: 0.1732 - val_multiclass_accuracy: 0.9809 - val_multiclass_loss: 0.0634 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9823 - binary_loss: 0.0564 - loss: 0.1992 - multiclass_accuracy: 0.9777 - multiclass_loss: 0.0714 - val_binary_accuracy: 0.9843 - val_binary_loss: 0.0485 - val_loss: 0.1728 - val_multiclass_accuracy: 0.9801 - val_multiclass_loss: 0.0630 - learning_rate: 5.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - binary_accuracy: 0.9829 - binary_loss: 0.0554 - loss: 0.1941 - multiclass_accuracy: 0.9787 - multiclass_loss: 0.0694 - val_binary_accuracy: 0.9761 - val_binary_loss: 0.0814 - val_loss: 0.2763 - val_multiclass_accuracy: 0.9724 - val_multiclass_loss: 0.0982 - learning_rate: 5.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - binary_accuracy: 0.9829 - binary_loss: 0.0533 - loss: 0.1876 - multiclass_accuracy: 0.9792 - multiclass_loss: 0.0671\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - binary_accuracy: 0.9829 - binary_loss: 0.0534 - loss: 0.1877 - multiclass_accuracy: 0.9792 - multiclass_loss: 0.0672 - val_binary_accuracy: 0.9846 - val_binary_loss: 0.0484 - val_loss: 0.1725 - val_multiclass_accuracy: 0.9808 - val_multiclass_loss: 0.0631 - learning_rate: 5.0000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - binary_accuracy: 0.9838 - binary_loss: 0.0525 - loss: 0.1834 - multiclass_accuracy: 0.9795 - multiclass_loss: 0.0655 - val_binary_accuracy: 0.9855 - val_binary_loss: 0.0460 - val_loss: 0.1622 - val_multiclass_accuracy: 0.9819 - val_multiclass_loss: 0.0590 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9839 - binary_loss: 0.0512 - loss: 0.1762 - multiclass_accuracy: 0.9807 - multiclass_loss: 0.0625 - val_binary_accuracy: 0.9835 - val_binary_loss: 0.0468 - val_loss: 0.1704 - val_multiclass_accuracy: 0.9788 - val_multiclass_loss: 0.0623 - learning_rate: 2.5000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - binary_accuracy: 0.9839 - binary_loss: 0.0513 - loss: 0.1773 - multiclass_accuracy: 0.9805 - multiclass_loss: 0.0630 - val_binary_accuracy: 0.9837 - val_binary_loss: 0.0524 - val_loss: 0.1830 - val_multiclass_accuracy: 0.9797 - val_multiclass_loss: 0.0659 - learning_rate: 2.5000e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - binary_accuracy: 0.9842 - binary_loss: 0.0501 - loss: 0.1731 - multiclass_accuracy: 0.9807 - multiclass_loss: 0.0615 - val_binary_accuracy: 0.9852 - val_binary_loss: 0.0443 - val_loss: 0.1584 - val_multiclass_accuracy: 0.9814 - val_multiclass_loss: 0.0577 - learning_rate: 2.5000e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - binary_accuracy: 0.9841 - binary_loss: 0.0515 - loss: 0.1748 - multiclass_accuracy: 0.9814 - multiclass_loss: 0.0617 - val_binary_accuracy: 0.9853 - val_binary_loss: 0.0447 - val_loss: 0.1583 - val_multiclass_accuracy: 0.9819 - val_multiclass_loss: 0.0575 - learning_rate: 2.5000e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9842 - binary_loss: 0.0508 - loss: 0.1761 - multiclass_accuracy: 0.9809 - multiclass_loss: 0.0627 - val_binary_accuracy: 0.9850 - val_binary_loss: 0.0471 - val_loss: 0.1747 - val_multiclass_accuracy: 0.9802 - val_multiclass_loss: 0.0640 - learning_rate: 2.5000e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9842 - binary_loss: 0.0505 - loss: 0.1726 - multiclass_accuracy: 0.9812 - multiclass_loss: 0.0610 - val_binary_accuracy: 0.9855 - val_binary_loss: 0.0436 - val_loss: 0.1560 - val_multiclass_accuracy: 0.9816 - val_multiclass_loss: 0.0566 - learning_rate: 2.5000e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - binary_accuracy: 0.9846 - binary_loss: 0.0498 - loss: 0.1703 - multiclass_accuracy: 0.9820 - multiclass_loss: 0.0603 - val_binary_accuracy: 0.9848 - val_binary_loss: 0.0453 - val_loss: 0.1591 - val_multiclass_accuracy: 0.9816 - val_multiclass_loss: 0.0573 - learning_rate: 2.5000e-04\n",
            "Epoch 23: early stopping\n",
            "Restoring model weights from the end of the best epoch: 8.\n",
            "\n",
            "[7] Evaluating...\n",
            "\n",
            "======================================================================\n",
            "EVALUATION RESULTS\n",
            "======================================================================\n",
            "\n",
            "Binary Accuracy:  0.9841\n",
            "Binary FPR:       0.0041\n",
            "Multiclass Accuracy: 0.9799\n",
            "Multiclass F1:    0.9781\n",
            "\n",
            "[8] Generating materials...\n",
            "\n",
            "================================================================================\n",
            "GENERATING REPORT MATERIALS\n",
            "================================================================================\n",
            "\n",
            "[1] Generating images...\n",
            "Saved: image1_training_history.png\n",
            "Saved: image2_binary_confusion_matrix.png\n",
            "Saved: image3_roc_curves.png\n",
            "Saved: image4_multiclass_confusion_matrix.png\n",
            "Saved: image5_class_distribution.png\n",
            "\n",
            "[2] Generating tables...\n",
            "\n",
            "Saved: table1_comparison.csv\n",
            "\n",
            "====================================================================================================\n",
            "BASELINE COMPARISON\n",
            "====================================================================================================\n",
            "+------------------------------------+-------------------+--------------------+-----------------+-------------+--------+-----------------------+-----------------+\n",
            "| Method                             |   Binary Accuracy |   Binary Precision |   Binary Recall |   Binary F1 |    FPR | Multiclass Accuracy   | Multiclass F1   |\n",
            "+====================================+===================+====================+=================+=============+========+=======================+=================+\n",
            "| Autoencoder (Miguel-Diez 2025)     |            0.984  |             0.975  |          0.982  |      0.9785 | 0.031  | N/A                   | N/A             |\n",
            "+------------------------------------+-------------------+--------------------+-----------------+-------------+--------+-----------------------+-----------------+\n",
            "| Isolation Forest                   |            0.912  |             0.885  |          0.924  |      0.9041 | 0.089  | N/A                   | N/A             |\n",
            "+------------------------------------+-------------------+--------------------+-----------------+-------------+--------+-----------------------+-----------------+\n",
            "| One-Class SVM                      |            0.875  |             0.832  |          0.901  |      0.8651 | 0.125  | N/A                   | N/A             |\n",
            "+------------------------------------+-------------------+--------------------+-----------------+-------------+--------+-----------------------+-----------------+\n",
            "| LSTM Only                          |            0.956  |             0.942  |          0.958  |      0.95   | 0.045  | 0.9230                | 0.9180          |\n",
            "+------------------------------------+-------------------+--------------------+-----------------+-------------+--------+-----------------------+-----------------+\n",
            "| Proposed Hybrid CNN-LSTM-Attention |            0.9841 |             0.9901 |          0.9567 |      0.9731 | 0.0041 | 0.9799                | 0.9781          |\n",
            "+------------------------------------+-------------------+--------------------+-----------------+-------------+--------+-----------------------+-----------------+\n",
            "\n",
            "Saved: table2_detailed_metrics.csv\n",
            "\n",
            "====================================================================================================\n",
            "PER-CLASS METRICS\n",
            "====================================================================================================\n",
            "+---------+-----------+-------------+----------+------------+---------------+-------+------+------+-------+\n",
            "| Class   |   Support |   Precision |   Recall |   F1-Score |   Specificity |    TP |   FP |   FN |    TN |\n",
            "+=========+===========+=============+==========+============+===============+=======+======+======+=======+\n",
            "| Benign  |     10500 |      0.9806 |   0.9966 |     0.9885 |        0.954  | 10464 |  207 |   36 |  4293 |\n",
            "+---------+-----------+-------------+----------+------------+---------------+-------+------+------+-------+\n",
            "| DoS     |      2250 |      0.9893 |   0.9884 |     0.9889 |        0.9981 |  2224 |   24 |   26 | 12726 |\n",
            "+---------+-----------+-------------+----------+------------+---------------+-------+------+------+-------+\n",
            "| Probe   |      1500 |      0.9967 |   1      |     0.9983 |        0.9996 |  1500 |    5 |    0 | 13495 |\n",
            "+---------+-----------+-------------+----------+------------+---------------+-------+------+------+-------+\n",
            "| Exploit |       600 |      0.878  |   0.6    |     0.7129 |        0.9965 |   360 |   50 |  240 | 14350 |\n",
            "+---------+-----------+-------------+----------+------------+---------------+-------+------+------+-------+\n",
            "| Malware |       150 |      0.9036 |   1      |     0.9494 |        0.9989 |   150 |   16 |    0 | 14834 |\n",
            "+---------+-----------+-------------+----------+------------+---------------+-------+------+------+-------+\n",
            "\n",
            "Saved: table3_architecture.csv\n",
            "\n",
            "================================================================================\n",
            "MODEL ARCHITECTURE\n",
            "================================================================================\n",
            "+----------------------+--------------------------------------------------------+\n",
            "| Component            | Details                                                |\n",
            "+======================+========================================================+\n",
            "| Input Dimension      | 22                                                     |\n",
            "+----------------------+--------------------------------------------------------+\n",
            "| CNN Layers           | Conv1D(64) -> Conv1D(128) with BatchNorm & Dropout     |\n",
            "+----------------------+--------------------------------------------------------+\n",
            "| LSTM Layers          | Bidirectional LSTM(128) with Dropout(0.3)              |\n",
            "+----------------------+--------------------------------------------------------+\n",
            "| Attention Mechanism  | Custom Attention with Dense & Softmax                  |\n",
            "+----------------------+--------------------------------------------------------+\n",
            "| Dense Layers         | Dense(256) -> Dense(128) with BatchNorm & Dropout(0.4) |\n",
            "+----------------------+--------------------------------------------------------+\n",
            "| Output Heads         | Binary + Multiclass (5 classes)                        |\n",
            "+----------------------+--------------------------------------------------------+\n",
            "| Total Parameters     | 390,151                                                |\n",
            "+----------------------+--------------------------------------------------------+\n",
            "| Trainable Parameters | 388,999                                                |\n",
            "+----------------------+--------------------------------------------------------+\n",
            "| Optimizer            | Adam                                                   |\n",
            "+----------------------+--------------------------------------------------------+\n",
            "| Learning Rate        | 0.0005                                                 |\n",
            "+----------------------+--------------------------------------------------------+\n",
            "\n",
            "================================================================================\n",
            "COMPLETE\n",
            "================================================================================\n",
            "\n",
            "======================================================================\n",
            "SUCCESS\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}